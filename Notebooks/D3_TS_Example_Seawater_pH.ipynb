{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "937960b0-ae1e-4278-b8be-49c93b7305aa",
   "metadata": {},
   "source": [
    "<img src=\"../Images/DSC_Logo.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5df1a-2a84-4210-ac43-e9afe3d4d685",
   "metadata": {},
   "source": [
    "# Seawater pH dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6309f-2a1a-4542-8ace-7fdebde7cea4",
   "metadata": {},
   "source": [
    "![sea](../Images/sea.jpg)\n",
    "\n",
    "*Image modified from Renan Brun, Pixabay*\n",
    "\n",
    "The dataset contains measured pH data and environmental factors such as temperature, salinity, and dissolved oxygen in the Bay of Palma. By training an LSTM model to predict pH and fill data gaps, the study extends pH time series beyond periods covered by direct measurements. \n",
    "\n",
    "**Original dataset and code:** https://github.com/agimenezromero/Coastal-pH-variability-reconstructed-through-neural-networks-the-coastal-Balearic-Sea-case-study\n",
    "\n",
    "**Related publication:** Flecha, S., Giménez-Romero, À., Tintoré, J., Pérez, F. F., Alou-Font, E., Matías, M. A., and Hendriks, I. E.: pH trends and seasonal cycle in the coastal Balearic Sea reconstructed through machine learning, Scientific Reports, 12, 12956, doi:10.1038/s41598-022-17253-5,     2022.\n",
    "\n",
    "Original data and code were modified for this notebook. The analyses presented here may not align with those published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a7c5a0-6d88-45a4-9918-33bbc8f29823",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install statsmodels seaborn scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b8ec0-eb69-407a-97e8-421462c8777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f9709-e772-4ea2-83e9-1bc524572bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # Suppress FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af592a06-8219-46a3-8d71-db89d219c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seawater_pH = pd.read_csv('../Datasets/seawater_pH.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20376c0-eaba-4869-a519-aba42ddbfffc",
   "metadata": {},
   "source": [
    "## **Exercise 1: Multivariate Time Series Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a19044-7fcf-48f5-9901-0bdd0fd3a7d9",
   "metadata": {},
   "source": [
    "## 1. Characteristics of Each Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316bfd3c-e385-4733-89a5-0096a6a290f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seawater_pH.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898827d3-874d-49dc-83b7-28624c30f352",
   "metadata": {},
   "source": [
    "**Exercise:** Print summary statistics of the dataset. Are the values reasonable from your experience? How many (less) values does the pH time series has compared to the other variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5875eab7-8475-4513-9209-28f28a79e134",
   "metadata": {},
   "outputs": [],
   "source": [
    "seawater_pH.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575b3e89-e85e-4e25-af7b-817debae76aa",
   "metadata": {},
   "source": [
    "Converting the 'Time' column to `datetime` format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18932d09-4bed-43a7-9dba-719d1ea4f3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seawater_pH[\"Time\"] = pd.to_datetime(seawater_pH[\"Time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ab70e8-885d-448b-b694-c080c8af5517",
   "metadata": {},
   "source": [
    "Plotting all time series in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab4b2a-8bc0-42b9-8c87-f7926bd3fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Plot Temperature and Oxygen on primary y-axis\n",
    "ax1.plot(seawater_pH['Time'], seawater_pH['Temperature'], 'b-', label='Temperature (°C)')\n",
    "ax1.plot(seawater_pH['Time'], seawater_pH['Oxygen'], '-', color='#8B008B', label='Oxygen (µmol/kg)')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Temperature & Oxygen', color='black')\n",
    "ax1.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "# Create a secondary y-axis for Salinity and pH\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(seawater_pH['Time'], seawater_pH['Salinity'], 'orange', label='Salinity (PSU)')\n",
    "ax2.plot(seawater_pH['Time'], seawater_pH['PH'], 'red', label='PH')\n",
    "ax2.set_ylabel('Salinity & PH', color='black')\n",
    "ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "# Combine legends from both axes\n",
    "lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cad9c0-2966-479d-a601-1e1299951b9e",
   "metadata": {},
   "source": [
    "## 2. Correlation and Cross-Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1867d-49dc-48a9-bca3-1717f6ce8e54",
   "metadata": {},
   "source": [
    "## 2.1 Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf43e1a2-4c78-4377-a0bf-b154761ea6ea",
   "metadata": {},
   "source": [
    "**Exercise:** Calculate the Pearson correaltion between all the variables in the dataset and plot the heatmap (using `seaborn` library). Identify which variables exhibit the strongest linear correlation with pH values, and investigate the nature of the relationships between these influential variables and pH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d493d74-70ce-4ff3-a36d-0c5d34608681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681a89f-4275-464f-9a07-14a40e87adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = seawater_pH.corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap of Influential Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c51440-5af8-42b0-8a9f-a21908b9bb9c",
   "metadata": {},
   "source": [
    "In the heatmap, pH shows the following correlations with other variables:\n",
    "\n",
    "- Strong negative correlation with Temperature (-0.91): As temperature rises, pH drops significantly.\n",
    "- Moderate positive correlation with Salinity (0.5): As salinity increases, pH also increases.\n",
    "- Strong positive correlation with Oxygen (0.88): Higher oxygen levels are linked to higher pH.\n",
    "\n",
    "These trends make sense. For instance, temperature often affects pH because warmer water holds less CO2, leading to lower pH. Oxygen and pH often rise together in areas of photosynthesis (more oxygen, higher pH)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af95e1c-6f48-4093-aa3f-3a96b97b12d2",
   "metadata": {},
   "source": [
    "## 2.2 Cross-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04019c82-b4fe-4e81-a5f5-ad128823f3f2",
   "metadata": {},
   "source": [
    "To investigate cross-correlation between pH and the influential variables we first need to remove data with missing pH values. Since data gaps impact the cross-correlation we want to avoid gaps as much as possible and hence define shared time periods for analyzing the cross-correlation individually for each time series pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796b445-c50f-4eec-bffe-a372dbd3123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pH_Oxygen = seawater_pH.dropna(subset=['PH','Oxygen'])\n",
    "pH_Salinity = seawater_pH.dropna(subset=['PH','Salinity'])\n",
    "pH_Temperature = seawater_pH.dropna(subset=['PH','Temperature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad5b664-6b4d-4da3-adbe-5022315fefcf",
   "metadata": {},
   "source": [
    "**Exercise:** Calculate the cross-correlation over a 200-day period for each time series pair using the `statsmodels` library. Analyze the results to determine the relationships between the variables. What insights can you derive from the cross-correlation analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbdf5cd-6801-4268-b24e-55c992eba00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import ccf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac8dcc-0b53-4483-ab95-55ca684edbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lag = 200\n",
    "\n",
    "# pH_Oxygen:\n",
    "cross_corr = ccf(pH_Oxygen['PH'], pH_Oxygen['Oxygen'])[:max_lag]\n",
    "plt.figure()\n",
    "plt.stem(range(max_lag), cross_corr)\n",
    "plt.title(\"Cross-Correlation between pH and Oxygen\")\n",
    "plt.xlabel(\"Lags\")\n",
    "plt.ylabel(\"Cross-Correlation\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# pH_Salinity:\n",
    "cross_corr = ccf(pH_Salinity['PH'], pH_Salinity['Salinity'])[:max_lag]\n",
    "plt.figure()\n",
    "plt.stem(range(max_lag), cross_corr)\n",
    "plt.title(\"Cross-Correlation between pH and Salinity\")\n",
    "plt.xlabel(\"Lags\")\n",
    "plt.ylabel(\"Cross-Correlation\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# pH_Temperature:\n",
    "cross_corr = ccf(pH_Temperature['PH'], pH_Temperature['Temperature'])[:max_lag]\n",
    "plt.figure()\n",
    "plt.stem(range(max_lag), cross_corr)\n",
    "plt.title(\"Cross-Correlation between pH and Temperature\")\n",
    "plt.xlabel(\"Lags\")\n",
    "plt.ylabel(\"Cross-Correlation\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae215a3-b762-4d89-9dd9-741c4d4c6043",
   "metadata": {},
   "source": [
    "The cross-correlation fluctuates significantly (from nearly 1 to nearly -1) suggesting that there are times in the year when pH and the influential variables are highly positively correlated, and other times when they are highly negatively correlated. They seem to be well suited for predicting pH values over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85354b9-a156-4b2c-a206-2bb8be1e7433",
   "metadata": {},
   "source": [
    "## **Exercise 2: Time Series Pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5234a9-c2f8-4c88-b664-e223b90fb3f8",
   "metadata": {},
   "source": [
    "## 2. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2782aaf1-38fa-4d16-98a6-03253637a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'PH' is the column of interest\n",
    "data_values = seawater_pH['PH']\n",
    "\n",
    "# Assume 'PH' is the column of interest and drop rows with NaN values for DBSCAN\n",
    "data_values_DBSCAN = data_values.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186f64ec-96cf-488e-bc4b-85a1b5124402",
   "metadata": {},
   "source": [
    "**Exercise:** As part of the preprocessing for modeling, outliers/anomalies in the pH time series were detected and manually removed. However, we want to test some outlier removal techniques and see how well we can detect what the authors identified as outliers/anomalies. \n",
    "\n",
    "Apply the Z-Score and Moving Average methods on 'data_values' for detecting outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df1df1-a09a-4126-a04f-f09187dedf15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9bc9da1-5a01-4e25-8f4e-fb81841ccb59",
   "metadata": {},
   "source": [
    "In the following we plot what the authors identified as outliers/anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be3f7c-fb23-481f-802e-692439ffa61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(seawater_pH[\"Time\"], seawater_pH[\"PH\"], s=1, c='b')\n",
    "plt.plot(seawater_pH[\"Time\"][(seawater_pH[\"Time\"] > datetime(2019, 8, 15)) & (seawater_pH[\"Time\"] < datetime(2019, 9, 1))], \n",
    "         seawater_pH[\"PH\"][(seawater_pH[\"Time\"] > datetime(2019, 8, 15)) & (seawater_pH[\"Time\"] < datetime(2019, 9, 1))],\n",
    "         color='r')\n",
    "plt.plot(seawater_pH[\"Time\"][(seawater_pH[\"Time\"] > datetime(2020, 6, 15)) & (seawater_pH[\"Time\"] < datetime(2020, 7, 1))], \n",
    "         seawater_pH[\"PH\"][(seawater_pH[\"Time\"] > datetime(2020, 6, 15)) & (seawater_pH[\"Time\"] < datetime(2020, 7, 1))],\n",
    "         color='r')\n",
    "plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dbf58e-8ff1-4c91-a496-9ae1534ba1ad",
   "metadata": {},
   "source": [
    "We remove the identified values from the dataset and plot the final pH time series for the prediction task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e8befe-de7c-44f1-a118-be4f854cae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = seawater_pH.drop(seawater_pH[(seawater_pH[\"Time\"] > datetime(2019, 8, 15)) & (seawater_pH[\"Time\"] < datetime(2019, 9, 1))].index)\n",
    "df = df.drop(df[(df[\"Time\"] > datetime(2020, 6, 15)) & (df[\"Time\"] < datetime(2020, 7, 1))].index)\n",
    "plt.scatter(df[\"Time\"], df[\"PH\"], s=1, c='b')\n",
    "plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d15090-4f79-4af3-8c1f-a799660de0f6",
   "metadata": {},
   "source": [
    "## 3. Data Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d192e57b-5b8e-4839-aed8-a5e135359df4",
   "metadata": {},
   "source": [
    "In the published code, the authors preselected data points where oxygen measurements are available prior to scaling. This approach prevents scaling the temperature and salinity time series on large datasets that will not be included in either the model fitting or the reconstruction task. This is particularly important given the limited availability of oxygen data compared to the other variables (compare the data availability for these time series plotted above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6f427-4227-48d1-9b90-5448c492b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[df[\"Oxygen\"].astype('str') != 'nan'][[\"Temperature\", \"Oxygen\", \"Salinity\", \"PH\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd4a3b9-2059-4534-bcae-3f3dd658c47a",
   "metadata": {},
   "source": [
    "**Exercise:** In their study, the authors used min-max normalization to scale the features (temperature, salinity, oxygen) and the pH values for better model fitting. It’s important to be careful when scaling the entire dataset in prediction tasks, because it can lead to data leakage and affect how well the model performs on new data. However, since this study is focused on reconstructing past pH values rather than predicting future ones, scaling the full dataset can be less of a concern. The model learns from historical data to estimate missing pH values, so using all available data helps improve accuracy. \n",
    "\n",
    "To implement this, use the `sklearn` `MinMaxScaler` to scale the dataset and name the scaled dataset 'data_n'. Keep the scaler for later.\n",
    "\n",
    "Note: The time series in this dataset contain missing values. Many common scaling methods, such as Min-Max scaling or standardization (Z-transformation), require a complete set of values. However, some libraries, like Scikit-learn in Python, have parameters to ignore NaN values. If you choose this option, only the available (non-NaN) values will be used to calculate the scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf96d40-a3ac-460f-9b65-2b61ff0c3429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3895510f-f894-47af-81b0-871df62a8a56",
   "metadata": {},
   "source": [
    "**Exercise:** Print the minimum and maximum of the scaled data to access the success of the scaling. Check out the `np.nanmin` function for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7122bc7-d076-49b6-aa41-7bd4f8ce9e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a728776",
   "metadata": {},
   "source": [
    "## **Exercise 3: Time Series Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1b57f-9bb8-4600-b64a-3acc45e5fecb",
   "metadata": {},
   "source": [
    "## 4. Sequence Generation for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cef30f-c31e-4416-8c5c-9fecc35c8f31",
   "metadata": {},
   "source": [
    "Extraction of sequences used for training and testing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbaec49-94db-44bd-be8f-1efacb63dda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 6\n",
    "n_features = data.shape[-1] - 1\n",
    "\n",
    "X = np.zeros((len(data_n) - window_size, window_size, n_features))\n",
    "Y = np.zeros(len(data_n) - window_size)\n",
    "for i in range(len(data_n) - window_size):\n",
    "    X[i] = data_n[i:window_size + i, 0:n_features]\n",
    "    Y[i] = data_n[window_size + i, -1]\n",
    "\n",
    "idxs = [i for i in range(len(X)) if np.any(np.isnan(X[i])) or np.isnan(Y[i])]\n",
    "X_new = np.delete(X, idxs, axis=0)\n",
    "Y_new = np.delete(Y, idxs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d5548-9ea0-454d-a96a-c4db5ba0f4fe",
   "metadata": {},
   "source": [
    "**Exercise:** In the study, the dataset subset containing also pH measurements was split into two parts. The purpose of the larger set (90% of the samples in 'X_new' and 'Y_new') was to teach the model to reconstruct pH values based on the features of temperature, oxygen, and salinity. The smaller set (10% of the samples in 'X_new' and 'Y_new') was used to evaluate the model’s performance and fine-tune its settings (adapt parameters) for better accuracy. \n",
    "\n",
    "Define a variable 'test_size' with the appropriate value to be used in the `train_test_split` function. Next, split the data into 'X_train'/'Y_train' and 'X_test'/'Y_test' and investigate the shape of the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb19fd-46a6-4ed3-9a74-1e19b5fa9e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec605c2-01f3-4cc5-96f2-588c269ae9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ebc046b-7349-4922-8c25-7a5e77502599",
   "metadata": {},
   "source": [
    "**Exercise:** For the reconstruction task, we now create dataset that will be used for model predictions. Create a Python list named 'features_list' that includes the names of the features utilized in this study to predict pH values. Then continue with the next code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537cdd69-6f23-41b3-9d52-9d66c06e8a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f63f09c-dd9e-40d3-b62f-8bfe3b66f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = df[features_list].values\n",
    "scaled_new = scaler.min_[:n_features] + data_new * scaler.scale_[:n_features]\n",
    "X_to_predict = np.zeros((len(scaled_new) - window_size, window_size, n_features))\n",
    "for i in range(len(scaled_new) - window_size):\n",
    "    X_to_predict[i] = scaled_new[i:window_size + i, 0:n_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c87c14-15e3-414c-957f-a069ab11d204",
   "metadata": {},
   "source": [
    "All datasets required for training and predicting with the LSTM model in this study have now been prepared. The following section will define the LSTM model, fit the model on available pH values and run predictions using the prepared datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282674b7-7bb4-4d8b-99dc-271640872bbb",
   "metadata": {},
   "source": [
    "## 5. LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7afc1e-8352-4549-8a20-f1813509c049",
   "metadata": {},
   "source": [
    "### Build Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85612c5e-1202-49dc-ba6e-21181bbc714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from time import time as time \n",
    "\n",
    "class PrintCrossPoint(Callback):\n",
    "    def __init__(self):\n",
    "        self.epoch_cross = \"\"\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch += 1\n",
    "        logs = logs or {}\n",
    "        current_train_loss = logs.get(\"loss\")\n",
    "        current_val_loss = logs.get(\"val_loss\")\n",
    "        if current_val_loss < current_train_loss:\n",
    "            if self.epoch_cross == \"\":\n",
    "                self.epoch_cross = self.epoch\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        print(\"Validation loss lower than training loss from epoch %s!\" % self.epoch_cross)\n",
    "\n",
    "class StopCrossPoint(Callback):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch += 1\n",
    "        logs = logs or {}\n",
    "        current_train_loss = logs.get(\"loss\")\n",
    "        current_val_loss = logs.get(\"val_loss\")\n",
    "        if current_val_loss < current_train_loss:\n",
    "            print(\"Training loss higher than validation loss from epoch %s!\" % self.epoch)\n",
    "            self.model.stop_training = True\n",
    "\n",
    "callback_1 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2, min_delta=0.00001)\n",
    "callback_2 = StopCrossPoint()   \n",
    "\n",
    "# Design network\n",
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(LSTM(3, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "model_LSTM.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02e87ca-210b-47c3-8425-5b8134eb7952",
   "metadata": {},
   "source": [
    "### Fit model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048048eb-87ce-4328-a833-b4b901c562d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit network\n",
    "history_LSTM = model_LSTM.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=500,\n",
    "    steps_per_epoch=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    verbose=0,\n",
    "    shuffle=False,\n",
    "    callbacks=[callback_1, callback_2]\n",
    ")\n",
    "\n",
    "# Run metrics\n",
    "t0 = time()\n",
    "time_elapsed_LSTM = time() - t0\n",
    "epochs_used_LSTM = len(history_LSTM.history['loss'])\n",
    "print(\"Finished in\", time_elapsed_LSTM, \"s using\", epochs_used_LSTM, \"epochs\")\n",
    "\n",
    "# Losses in percent\n",
    "final_train_loss_LSTM = (history_LSTM.history['loss'][-1] * 100)\n",
    "final_val_loss_LSTM = (history_LSTM.history['val_loss'][-1] * 100)\n",
    "init_train_loss_LSTM = (history_LSTM.history['loss'][0] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d6de67-8b32-4f1b-8852-72fdd0f2d82c",
   "metadata": {},
   "source": [
    "### Predict model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb1566",
   "metadata": {},
   "source": [
    "**Exercise:** Use the model to predict and reconstruct pH values, and assign the predicted output values to the variable 'y_pred_LSTM'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c810bee-99c4-4e16-a535-49f91fa542a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ca62d5b",
   "metadata": {},
   "source": [
    "Finally, we rescale `y_pred_LSTM` to return it to the original scale of the pH values. \n",
    "\n",
    "Note: Although `scaler.inverse_transform` can be used for this purpose, it's important to note that the `MinMaxScaler` was fitted on the multivariate original data, which included four features: Temperature, Salinity, Oxygen, and pH. As a result, the method expects an input with four columns. When attempting to pass `y_pred_LSTM`, the shapes do not match, leading to a shape mismatch error. Therefore, we directly apply the scaling formula to convert the normalized pH predictions back to their original scale, using the parameters from the scaler that correspond specifically to the pH values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f50cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_noscale_LSTM = (y_pred_LSTM - scaler.min_[-1]) / scaler.scale_[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cea2a0-e2e8-4648-83b2-cd4e56cea9db",
   "metadata": {},
   "source": [
    "## 6. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f469dffb",
   "metadata": {},
   "source": [
    "Linear regression was used in the study to examine the long-term trends and seasonal changes of pH levels in the coastal Balearic Sea based on the reconstructed pH data obtained, with trends calculated using yearly intervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f40e44-5e76-4964-8308-db9a854840cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign predictions to the appropriate locations in y_pred_noscale_LSTM\n",
    "y_pred_noscale_LSTM[:, 0][df[\"PH\"][window_size:].astype('str') != 'nan'] = df[\"PH\"][window_size:][df[\"PH\"][window_size:].astype('str') != 'nan'].values\n",
    "\n",
    "# Calculating time differences in years\n",
    "time_delta = df[\"Time\"][window_size:] - df[\"Time\"][window_size]\n",
    "time_years = np.array([(item / np.timedelta64(1, 'm')) / (60 * 24 * 365) for item in time_delta])\n",
    "\n",
    "# Preparing data for linear regression\n",
    "Y_TREND = y_pred_noscale_LSTM[y_pred_noscale_LSTM.astype('str') != 'nan']\n",
    "X_TREND = time_years[y_pred_noscale_LSTM[:, 0].astype('str') != 'nan']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f894a-a95a-4625-b194-9e6126b80934",
   "metadata": {},
   "source": [
    "**Exercise:** Y_TREND contains the reconstructed pH values and X_TREND containes the corresponding time in yearly intervals. Fit a linear regression model to the reconstructed pH time series data using the `statsmodels` library. What is the slope of the trend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed4e7e-0604-4b3b-835b-15b5b59c2bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1065bebe-5f66-4e63-9e39-a5f26464b5fa",
   "metadata": {},
   "source": [
    "## 7. Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94517a9-2602-4821-9612-1d92683e0f15",
   "metadata": {},
   "source": [
    "In the following, we visualize the model results along with the pH reconstructions. Please note that the figure in the publication represent a different neural network model, which has demonstrated slightly better accuracy for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b333f-0c14-4e84-95c5-a33ebca344ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Subplot 1: Loss functions\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history_LSTM.history['loss'], label='Train loss', lw=3, c='black')\n",
    "plt.plot(history_LSTM.history['val_loss'], label='Validation loss', lw=3, c='grey')\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=30, labelpad=10)\n",
    "plt.ylabel(\"Loss (MSE)\", fontsize=30, labelpad=10)\n",
    "plt.text(epochs_used_LSTM * 0.5, init_train_loss_LSTM * 0.006,\n",
    "         \"Final train loss: %.2f%%\" % final_train_loss_LSTM, fontsize=16)\n",
    "plt.text(epochs_used_LSTM * 0.5, init_train_loss_LSTM * 0.005,\n",
    "         \"Final val loss: %.2f%%\" % final_val_loss_LSTM, fontsize=16)\n",
    "plt.legend(fontsize=20)\n",
    "\n",
    "# For evaluating model fit: Receive prediction of the entire dataset (training + testing) to compare with observations\n",
    "yhat_LSTM = model_LSTM.predict(X_new) \n",
    "\n",
    "# Subplot 2: Predicted pH time series in the training process (orange) and observation (blue)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(yhat_LSTM, Y_new, s=20, c='grey')\n",
    "plt.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), color='black', lw=3)\n",
    "plt.xlabel(\"Predicted values\", fontsize=30, labelpad=10)\n",
    "plt.ylabel(\"True values\", fontsize=30, labelpad=10)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "# Subplot 3: Predicted vs. observed pH values\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(Y_new, lw=3, c='b', label='Measured')  # Observed values in blue\n",
    "plt.plot(yhat_LSTM, lw=3, c='orange', label='Predicted')  # Predicted values in orange\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlabel(\"Training data sequence\", fontsize=30, labelpad=10)\n",
    "plt.ylabel(r\"pH$_\\mathrm{T}$ (scaled)\", fontsize=30, labelpad=10)\n",
    "\n",
    "# Subplot 4\n",
    "plt.subplot(2, 2, 4)\n",
    "df_to_save = df.loc[:, (\"Time\", \"PH\")]\n",
    "df_to_save[\"DataType\"] = [\"\" for i in range(len(df_to_save))]\n",
    "\n",
    "for i in range(6, len(df_to_save[6:])):\n",
    "    if np.isnan(df_to_save[\"PH\"].iloc[i]):\n",
    "        df_to_save[\"PH\"].iloc[i] = y_pred_noscale_LSTM[i][0]\n",
    "        df_to_save[\"DataType\"].iloc[i] = \"Prediction\"\n",
    "    else:\n",
    "        df_to_save[\"DataType\"].iloc[i] = \"Observation\"\n",
    "\n",
    "plt.scatter(df_to_save[\"Time\"][df_to_save[\"DataType\"] == \"Observation\"], \n",
    "            df_to_save[\"PH\"][df_to_save[\"DataType\"] == \"Observation\"], s=1, lw=3, c='b', label='Measured')\n",
    "plt.scatter(df_to_save[\"Time\"][df_to_save[\"DataType\"] == \"Prediction\"], \n",
    "            df_to_save[\"PH\"][df_to_save[\"DataType\"] == \"Prediction\"], s=1, lw=3, c='r', label='Reconstructed')\n",
    "time_delta_plot = df[\"Time\"] - df[\"Time\"][0]\n",
    "time_years_plot = np.array([(item / np.timedelta64(1, 'm')) / (60 * 24 * 365) for item in time_delta_plot])\n",
    "plt.plot(df_to_save[\"Time\"], slope_LSTM * time_years_plot + intercept_LSTM, color='k', lw=3, \n",
    "         label=r'y=%.4fx + %.4f' % (slope_LSTM, intercept_LSTM))\n",
    "plt.xticks(fontsize=16, rotation=30)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlabel(\"Time [years]\", fontsize=30)\n",
    "plt.ylabel(r\"pH$_\\mathrm{T}$\", fontsize=30)\n",
    "\n",
    "# Custom legend without markers\n",
    "plt.figtext(0.3, 0, 'Measured', fontsize=30, ha='center', color='blue')  # Centered below the plots\n",
    "plt.figtext(0.5, 0, 'Predicted', fontsize=30, ha='center', color='orange')  # Centered below the plots\n",
    "plt.figtext(0.7, 0, 'Reconstructed', fontsize=30, ha='center', color='red')  # Centered below the plots\n",
    "\n",
    "# Show the plot\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.3, bottom=0.15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d3977-bcf0-485e-882e-19b66b866fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
