{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ff063b-385b-4894-a9b8-53ce4365bd9d",
   "metadata": {},
   "source": [
    "<img src=\"../Images/DSC_Logo.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30026fa9-017d-490c-a08b-0b3ae2376adc",
   "metadata": {},
   "source": [
    "# Time Series Theory in Python - Part 1: Exploratory Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae061fb-5200-490e-bbf3-6b7bf8d5ea3f",
   "metadata": {},
   "source": [
    "This notebook focuses on introducing basic concepts of time series analysis for univariate time series and then extending to some specific analysis for multivariate time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a8401-c49e-416d-98a6-4d973349418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from pandas.plotting import lag_plot\n",
    "\n",
    "from numpy.random import normal\n",
    "from numpy import random\n",
    "\n",
    "from statsmodels.graphics.tsaplots import month_plot\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.tsa.stattools import ccf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from PythonTsa.datadir import getdtapath\n",
    "dtapath=getdtapath()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400c68f-158e-4f11-96ed-41e0bc442217",
   "metadata": {},
   "source": [
    "## 1. Stationarity and Linear Behaviour\n",
    "\n",
    "Stationarity is a fundamental concept in time series analysis, indicating that a time series has constant mean and variance over time. Understanding stationarity is crucial because many forecasting methods rely on the assumption that the data is stationary to produce accurate predictions. Recognizing whether data is stationary or non-stationary helps in selecting the appropriate analytical techniques and models.\n",
    "\n",
    "In contrast to non-stationary, nonlinearity refers to relationships in the data that cannot be accurately described by linear models. These two concepts can coexist, as a nonstationary time series may exhibit non-linear patterns, which can complicate the analysis. Additionally, nonstationarity can obscure underlying non-linear relationships, making it challenging to discern true patterns.\n",
    "\n",
    "The following code block demonstrates non-stationarity due to both an increasing trend and seasonal fluctuations in the synthetic time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ad60c-0081-48dd-b5da-9facd92520ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic time series data\n",
    "time = pd.date_range(start=\"2020-01-01\", periods=100)\n",
    "trend = np.arange(100)  # Linear trend as a NumPy array\n",
    "seasonal = [10 * np.sin(i / 5) for i in range(100)]  # Seasonal component\n",
    "noise = np.random.normal(0, 2, size=100)  # Random noise\n",
    "\n",
    "# Combine trend, seasonal, and noise components into one array\n",
    "data = trend + seasonal + noise\n",
    "\n",
    "# Create DataFrame\n",
    "series = pd.Series(data, index=time)\n",
    "\n",
    "# Plot the time series\n",
    "series.plot()\n",
    "plt.title(\"Synthetic time series with trend and seasonality\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a889d-59ff-4176-b802-845222ec0fcc",
   "metadata": {},
   "source": [
    "## 2. Autocorrelation\n",
    "\n",
    "Autocorrelation is a statistical measure that assesses how a time series is correlated with its past values. Essentially, it tells us whether and how current values in the series are influenced by previous values at different time intervals (or lags). Autocorrelation therefore helps identify patterns in the data (e.g. trends; seasonality), and reveals the strength and direction of relationships over time. Many time series models, like ARIMA, use autocorrelation as a key component in their calculations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee369900-1f08-4cc8-bcc7-2eb9cb17a5e2",
   "metadata": {},
   "source": [
    "## 2.1 Autocorrelation Function (ACF)\n",
    "\n",
    "The ACF Plot measures the correlation between the current observation in a time series and its past values. It illustrates how current values are related to their previous values over different time intervals, highlighting any patterns or dependencies in the data. Analyzing the ACF is essential for identifying the presence of trends or seasonality and determining the appropriate order of the moving average component in models like ARIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08801172-2327-4f35-adbb-54f17edbce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(series, lags=30, alpha=0.05) # alpha=0.05 is the default)\n",
    "plt.title(\"ACF of the Time Series\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f6c82-9b1a-4d85-ab00-208ec625fd48",
   "metadata": {},
   "source": [
    "The **autocorrelation values** (represented by the bars) gradually decrease over time. At lag 0, the autocorrelation is always 1 (by definition), showing a perfect correlation with itself. The presence of a clear upward trend in the time series contributes to the high autocorrelation values that decay slowly with increasing lags. This slow decay is typical of a non-stationary series with a trend, indicating that past values continue to influence future values significantly.\n",
    "\n",
    "The first several lags (up to around lag 10) exhibit high autocorrelation values. Most of these early lags extend beyond the shaded blue area, which represents the 95% **confidence interval**, indicating that they are statistically significant. This significance means that past values have a substantial influence on current values. As the lags increase, the confidence bands widen, signifying increased uncertainty in the autocorrelation estimates. A wide confidence band suggests that the time series exhibits non-consistent behavior with significant changes over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e2053-a371-48d1-b153-5043356e09c6",
   "metadata": {},
   "source": [
    "## 2.2 Partial Autocorrelation Function (PACF)\n",
    "\n",
    "The PACF Plot helps identify the direct relationship between the current observation in a time series and its past values while excluding the influence of intermediate lagged values. It shows how strongly the current observation is related to a specific past observation, providing insights into the unique connection between them. Analyzing the PACF is crucial for determining the appropriate order of the autoregressive component in models like ARIMA, which improves forecasting accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05ebfb-3e94-47ce-9f26-5e271e51df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(series, lags=30)\n",
    "plt.title(\"PACF of the Time Series\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f365c0-eb42-4cca-bf72-60ea00d159ea",
   "metadata": {},
   "source": [
    "Only the first and second lags exhibit significant correlations, as indicated by their position outside the shaded confidence interval. This suggests that only these two lags have a direct influence on the current value of the series. Although the time series displays a trend (indicating non-stationarity), the PACF still identifies significant values primarily at the first two lags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad6eba8-3b04-41b4-9abe-1ab8a6283c66",
   "metadata": {},
   "source": [
    "## 2.3 Lag Plot\n",
    "\n",
    "The Lag Plot provides a scatter plot that compares the values of a time series to its lagged values, helping to identify any linear relationships between them. Linear relationships observed in a Lag Plot suggest dependence on past values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de70071-c668-446f-bacd-de966d72337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot(series)\n",
    "plt.title('Lag Plot of the Time Series')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71885819-de9a-4dc3-aa25-3dda1805677f",
   "metadata": {},
   "source": [
    "An upward trend in a Lag Plot can mean that current values increase with past values. However, this trend can also result from seasonality, where the values fluctuate consistently within certain time frames. To tell the difference, we can look for repeated patterns at specific lags (which indicates seasonality) versus a steady increase over time (which indicates a trend)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c763d58b-c49f-4f55-8c36-69eaaed4a676",
   "metadata": {},
   "source": [
    "**Exercise:** Generate synthetic time series data without a trend and analyze it for stationarity by examining its autocorrelation properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb8521-a1db-4685-acd6-e33b3277af3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad473cb6-5f4b-4520-ae78-a60739727abe",
   "metadata": {},
   "source": [
    "### **Example 1: Chinese Quarterly GDP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80038210-53ae-4e97-b731-d9eae08dd150",
   "metadata": {},
   "source": [
    "The Chinese Quarterly GDP time series exhibits a clear increasing trend along with increasing seasonal effects (non-stationary behavior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24bc5fc-f325-42d2-bcaa-c50d32925fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "gdp = pd.read_csv(dtapath + 'gdpquarterlychina1992.1-2017.4.csv',header=0)\n",
    "dates = pd.date_range(start='1992',periods=len(gdp),freq='QE')\n",
    "gdp.index=dates\n",
    "\n",
    "# Plot the time series\n",
    "gdp.plot()\n",
    "plt.title('Chinese Quarterly GDP 1992-2017')\n",
    "plt.ylabel('billions of RMB')\n",
    "plt.show()\n",
    "\n",
    "# Plot ACF\n",
    "plot_acf(gdp, lags=60)\n",
    "plt.title(\"ACF of the Time Series\")\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF\n",
    "fig = plt.figure()\n",
    "plot_pacf(gdp, lags=17)\n",
    "plt.title(\"PACF of the Time Series\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Lag Plot\n",
    "lag_plot(gdp)\n",
    "plt.title('Lag Plot of the Time Series')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0fb5b-c8a7-44c7-9837-3c892b1d57f3",
   "metadata": {},
   "source": [
    "### **Example 2: Quarterly Exchange Rates of GBP to NZ Dollar**\n",
    "\n",
    "The time series shows relatively complex fluctuations with clear phases of increase, decrease and then increase again (non-stationary behavior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e795b9-0f7e-43dc-b207-4fd707894d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the exchange rate data\n",
    "exchange = pd.read_csv(dtapath + 'ExchRate NZ per UK.txt', header=0)\n",
    "\n",
    "# Create a date range starting from 1991 with quarterly frequency\n",
    "dates = pd.date_range('1991', periods=len(exchange), freq='QE')\n",
    "\n",
    "# Set the index to the created date range\n",
    "exchange.index = dates\n",
    "\n",
    "# Create a time series from the 'xrate' column\n",
    "exchange = pd.Series(exchange['xrate'])\n",
    "\n",
    "# Plot the time series\n",
    "exchange.plot()\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Exchange rate')\n",
    "plt.title('Exchange Rate NZ per UK (1991 onward)')\n",
    "plt.show()\n",
    "\n",
    "# Plot ACF\n",
    "fig = plt.figure()\n",
    "plot_acf(exchange, lags=17)\n",
    "plt.title(\"ACF of the Time Series\")\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF\n",
    "fig = plt.figure()\n",
    "plot_pacf(exchange, lags=17)\n",
    "plt.title(\"PACF of the Time Series\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Lag Plot\n",
    "lag_plot(exchange)\n",
    "plt.title('Lag Plot of the Time Series')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a482dda-127f-45ed-8bd6-f7d0b2642323",
   "metadata": {},
   "source": [
    "### **Example 3: The NAO Index Since January 1950**\n",
    "\n",
    "The time series is the monthly mean North Atlantic Oscillation (NAO) index since January 1950. From the time series plot and plots showing and autocorrelation, we could say that the time series appears stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee31635-774b-47ff-a448-2039d9779ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NAO dataset\n",
    "nao = pd.read_csv(dtapath + 'nao.csv', header=0)\n",
    "\n",
    "# Create a time index\n",
    "timeindex = pd.date_range('1950-01', periods=len(nao), freq='ME')\n",
    "nao.index = timeindex\n",
    "\n",
    "# Extract NAO index as a Series\n",
    "naots = nao['index']  # Ensure 'index' corresponds to the correct column name\n",
    "\n",
    "# Plot the NAO index time series\n",
    "naots.plot(title='NAO Index Time Series', xlabel='Date', ylabel='NAO Index')\n",
    "plt.show()\n",
    "\n",
    "# Plot ACF\n",
    "fig = plt.figure()\n",
    "plot_acf(naots, lags=50)\n",
    "plt.title(\"ACF of the Time Series\")\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF\n",
    "fig = plt.figure()\n",
    "plot_pacf(naots, lags=50)\n",
    "plt.title(\"PACF of the Time Series\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Lag Plot\n",
    "lag_plot(naots)\n",
    "plt.title('Lag Plot of the Time Series')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8beedb-1cdc-4840-8a06-5c47281f5992",
   "metadata": {},
   "source": [
    "## 3. Random walks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a876b5-1cdc-4e44-adf4-8249480ceab0",
   "metadata": {},
   "source": [
    "A random walk is a mathematical model that describes a path consisting of a series of random steps. In a random walk, the next value is the sum of the previous value and a random change, making it useful for modeling uncertainty and variability.\n",
    "\n",
    "In the context of Earth Sciences, it can be used to represent processes like the dispersal of pollutants in the atmosphere or the movement of animal populations in ecology, where the future position or state is influenced by random changes rather than a predictable trend. \n",
    "\n",
    "Three simulated paths (time plots) of the standard normal random walk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3debf9-34f2-40ca-b053-b593a646bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1357)\n",
    "a=normal(size=300); b=normal(size=300); c=normal(size=300)\n",
    "x = np.cumsum(a); y=np.cumsum(b); z=np.cumsum(c)\n",
    "xyz = pd.DataFrame({'x': x, 'y': y, 'z': z})\n",
    "xyz.index = range(1,301)\n",
    "xyz.plot(style=['-', '--', ':']); plt.show() # style means matplotlib line style per column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f78a7-f441-4d92-af02-c8acf13b074f",
   "metadata": {},
   "source": [
    "## 4. White noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e4c913-10f0-47ae-b9fc-2b284f4b47b7",
   "metadata": {},
   "source": [
    "## 4.1 Simulating a Gaussian white noise\n",
    "\n",
    "Gaussian white noise is a random signal that has equal intensity at all frequencies, making it useful for representing random fluctuations in data that follow a normal distribution. \n",
    "\n",
    "To intuitively test whether a stationary time series is white noise, we can examine its ACF plot: if the plot resembles this figure, we are inclined to conclude that the time series is indeed white noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50878bef-8059-495d-b3ee-22e5b11e4a62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed(135) # for repeat\n",
    "x = random.normal(loc=0, scale=1, size=1000)\n",
    "xts = pd.Series(x)\n",
    "xts.plot(); plt.xlabel('Time')\n",
    "plt.ylabel('Simulated white noise')\n",
    "plt.show()\n",
    "\n",
    "plot_acf(xts, lags=30) # plotting ACF\n",
    "plt.title(\"ACF of the Time Series\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac19be1f-aeca-43d2-a4e6-5c33f636fe99",
   "metadata": {},
   "source": [
    "## 4.2 Chaos like a white noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfeddad-92ae-42fd-966f-9954193c7efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty pandas Series with a float data type\n",
    "x = pd.Series(dtype=float)\n",
    "\n",
    "# Start value for y\n",
    "y = 0.3\n",
    "\n",
    "# Use a for loop to generate the values\n",
    "for t in range(1, 501):\n",
    "    y = 4.0 * y * (1 - y)  # Logistic map equation\n",
    "    x.loc[t-1] = y  # Assigning the value at index t-1\n",
    "\n",
    "# Set the index of the series to be in the range 1 to 500\n",
    "x.index = range(1, 501)\n",
    "\n",
    "# Display the result\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9b2d1-bc96-4b06-a834-2459a9be8a66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.plot()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Simulated Chaos')\n",
    "plt.show()\n",
    "\n",
    "plot_acf(x, lags=30) # plotting ACF\n",
    "plt.title(\"ACF of the Time Series\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6246427-acbd-4079-938c-4335d6c3298d",
   "metadata": {},
   "source": [
    "## 4.3 White noise test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e7c82-7bf4-47c3-9e36-c7a6f3e687ed",
   "metadata": {},
   "source": [
    "How to statistically test whether a stationary time series is white noise:\n",
    "\n",
    "If the ACF values for lags other than zero are close to zero and fall within the confidence intervals, the series is likely white noise. However, if any ACF values are significantly different from zero, it indicates that the series is not white noise due to the presence of correlations.\n",
    "\n",
    "Besides visual analysis, statistical methods for testing whether a stationary time series is a white noise or not exist. The Ljung-Box statistic checks if there are significant autocorrelations at multiple lags. The null hypothesis is that the data are independently distributed (i.e., white noise). If the p-value is low (commonly less than 0.05), we reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a31743e-5eed-4fe1-ba0d-d828cf62c665",
   "metadata": {},
   "source": [
    "### **Example 2 [continues]: Quarterly Exchange Rates of GBP to NZ Dollar**\n",
    "\n",
    "The quarterly exchange rates of GBP to NZD are a financial time series. We often analyze their logarithms because this stabilizes the average and variability, reduces trends, and makes the data more normal. This is important due to the volatility in financial data. Logging also highlights relative changes instead of absolute values, which is key for understanding percentage returns and growth rates in finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a898b2a-b088-4213-a009-53ad74d103bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logarithm of the series:\n",
    "logxts = np.log(exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be2649-b969-405d-8715-f6788ed2f3cf",
   "metadata": {},
   "source": [
    "We need to difference the time series to achieve stationarity before testing for white noise. Differencing is more closely investigated in notebook C2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a8b7b1-90f2-4ad5-8f7a-91746b46b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogxts = logxts.diff(1).dropna()  # Remove NaN values resulting from differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb2f24-b4b5-4027-a091-b7c53f3ac15c",
   "metadata": {},
   "source": [
    "Test for white noise:\n",
    "\n",
    "The time series differences of the log quarterly exchange rates seems to have neither trend nor seasonality and the ACF plot looks similar to the ACF plot of a white noise. The p-values from the Ljung-Box test suggest that the time series behaves like white noise at higher lags since the p-values for those lags are larger than 0.05, indicating no significant autocorrelation. However, since the earlier p-values (for the first lags) indicate some level of autocorrelation, we can't definitively say that the entire differentiated time series is purely white noise. It is approaching white noise behavior but might still have some dependencies at the initial lags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c73eb32-b820-4fa3-9736-c2c144563dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the differenced logarithmic series\n",
    "dlogxts.plot(marker='o', markersize=5)\n",
    "plt.title('Difference of Logarithm of the ExchRate NZ per UK')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Difference of Log Price')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the ACF\n",
    "plot_acf(dlogxts, lags=17)\n",
    "plt.title(\"ACF\")\n",
    "plt.show()\n",
    "\n",
    "# Perform the Ljung-Box test for residuals\n",
    "    # Calculate Ljung-Box test statistics and p-values\n",
    "ljung_box_results = acorr_ljungbox(dlogxts, lags=35, return_df=True)\n",
    "    # Create a plot for the p-values\n",
    "plt.figure()\n",
    "plt.plot(ljung_box_results['lb_pvalue'], marker='o', linestyle='-', color='b')\n",
    "plt.axhline(y=0.05, color='r', linestyle='--')  # 5% significance level\n",
    "plt.title('Ljung-Box Test P-Values')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('P-Value')\n",
    "plt.xticks(np.arange(0, 36, 1))\n",
    "plt.xticks(ljung_box_results.index)  # Set x-ticks to all lags\n",
    "plt.gca().set_xticklabels([str(int(x)) if x % 2 == 0 else '' for x in ljung_box_results.index])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c725f2aa-0d4f-4538-8ce6-b8425ea9f67a",
   "metadata": {},
   "source": [
    "## 5. Time Series Components: Time Series Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f57f7-c4f6-4bee-946f-7cb4bb9a2f7a",
   "metadata": {},
   "source": [
    "In practice, many real-world time series exhibit either deterministic seasonality or a deterministic trend component. Some of them may have both. After extracting the trend and seasonal components from a time series, the remainder is its random (variation) component. Decomposing a time series can be quite beneficial for gaining insights into its structure and can serve as a pre-processing step for modeling (see notebook C3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fc45fe-fc46-4c43-9e9b-25870eeeeaa1",
   "metadata": {},
   "source": [
    "### **Example 1 [continued]: Chinese Quarterly GDP** \n",
    "\n",
    "The time series exhibits an increasing trend and seasonality. In the following, both additive and multiplicative decomposition methods are employed to create a time series where the residuals are like those of a stationary series. However, the results of the decomposition are unsatisfactory, as certain structures within the time series remain concealed. Instead, the Holt-Winters smoothing technique, which is particularly well-suited for macroeconomic data, would likely yield better results in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1cd26-2963-45d2-84a1-da83f1de1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Decomposition\n",
    "xdeca = seasonal_decompose(gdp, model='additive')\n",
    "xdecm = seasonal_decompose(gdp, model='multiplicative')\n",
    "\n",
    "# Plot decompositions\n",
    "xdeca.plot()\n",
    "plt.show()\n",
    "xdecm.plot()\n",
    "plt.show()\n",
    "\n",
    "# Lag Plot for Residuals\n",
    "fig = plt.figure()\n",
    "lag_plot(xdeca.resid, ax=fig.add_subplot(211))\n",
    "plt.title('Additive Decomposition')\n",
    "lag_plot(xdecm.resid, ax=fig.add_subplot(212))\n",
    "plt.title('Multiplicative Decomposition')\n",
    "plt.show()\n",
    "\n",
    "# Process Residuals\n",
    "xdecmResid = xdecm.resid.dropna()  # Drop NA values\n",
    "\n",
    "# Plot ACF and PACF for the residuals\n",
    "plot_acf(xdecmResid, lags=20)\n",
    "plt.title('ACF of Residuals')\n",
    "plt.show()\n",
    "plot_pacf(xdecmResid, lags=20)\n",
    "plt.title('PACF of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524d5d3-8466-4afd-8a66-e80ed7b7b2a4",
   "metadata": {},
   "source": [
    "### **Example 4: Australian Employed Total Persons**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a4f79f-00f7-4c4e-8aa3-49da1a21c6f2",
   "metadata": {},
   "source": [
    "The \"Australia Employed Total Persons\" dataset tracks the monthly total number of employed people in Australia from February 1978 to November 2018. Initially, the time series shows a steady upward trend with no apparent seasonality. However, when zooming in on data from January 2013 to January 2017, clear seasonal patterns emerge, repeating every year. Seasonal plots confirm that these patterns are consistent with little variation over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b5f2f-3c99-4237-9a11-608448ae24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data file path\n",
    "dtapath = getdtapath()\n",
    "\n",
    "# Load the Excel file containing employment data\n",
    "aul = pd.read_excel(dtapath + 'AustraliaEmployedTotalPersons.xlsx', header=0)\n",
    "\n",
    "# Create a time index starting from February 1978 with monthly frequency\n",
    "timeindex = pd.date_range('1978-02', periods=len(aul), freq='ME')\n",
    "aul.index = timeindex\n",
    "\n",
    "# Extract the 'EmployedP' column for analysis\n",
    "aults = aul['EmployedP']\n",
    "\n",
    "# Plot the time series\n",
    "aults.plot()\n",
    "plt.title('Total Employed Persons in Australia')\n",
    "plt.ylabel('Number of Persons Employed')\n",
    "plt.xlabel('Date')\n",
    "plt.show()\n",
    "\n",
    "# Graph time series plot from January 2013 to January 2017\n",
    "aults['2013-01':'2017-01'].plot()\n",
    "plt.title('Total Employed Persons (2013-01 to 2017-01)')\n",
    "plt.ylabel('Number of Persons Employed')\n",
    "plt.xlabel('Date')\n",
    "plt.show()\n",
    "\n",
    "# Plot seasonal plots\n",
    "month_plot(aults)\n",
    "plt.title('Monthly Employment Data Seasonal Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd70d3c-662d-4d17-8fb2-91fd8b9e7f82",
   "metadata": {},
   "source": [
    "Using an additive model to decompose the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e798f7-8b1f-4865-8ba7-90c8c8d99b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose the time series using an additive model (Time series = Trend + Seasonality + Residual)\n",
    "aultsdeca = seasonal_decompose(aults, model='additive', period=12)\n",
    "aultsdeca.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b083c-064f-426a-8fff-701fc7cca4ae",
   "metadata": {},
   "source": [
    "After decomposition, the residuals should ideally represent the random noise, indicating successful removal of trends and seasonal patterns. We can adress this, for example, by plotting the ACF of the residuals to ensure residuals exhibit insignificant autocorrelation, indicating no remaining patterns; by testing for normality to confirm the residuals are distributed as expected random noise; by checking rolling means and standard deviations to ensure stationarity, as constant values indicate effective decomposition.\n",
    "\n",
    "We see that the residuals are mostly stationary, though there are seasonal correlations at specific lags, and represent represent the random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca002cb3-65ec-450f-a555-106c7ff9a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract residuals from the decomposition and drop NaN values\n",
    "aultsdecaResid = aultsdeca.resid.dropna()\n",
    "\n",
    "# Plot ACF of the residuals\n",
    "plot_acf(aultsdecaResid, lags=48)\n",
    "plt.title('ACF of Residuals')\n",
    "plt.show()\n",
    "\n",
    "# Rolling mean and standard deviation\n",
    "rolm = aultsdecaResid.rolling(window=36, center=True).mean()\n",
    "rolstd = aultsdecaResid.rolling(window=36, center=True).std()\n",
    "\n",
    "# Plotting residuals, rolling mean, and rolling std\n",
    "plt.plot(aultsdecaResid, label='Decomposed Residuals')\n",
    "plt.plot(rolm, label='Rolling Mean', linestyle='--')\n",
    "plt.plot(rolstd, label='Rolling Std', linestyle=':', c='red')\n",
    "plt.title('Residuals of Australian Employed Persons')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Q-Q plot to test for normality\n",
    "plt.figure()\n",
    "sm.qqplot(aultsdecaResid, line='q', ax=plt.gca())\n",
    "plt.title('Q-Q Plot (Sample vs Theoretical Quantiles)')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24786d18-a438-4815-991b-a2704880fb72",
   "metadata": {},
   "source": [
    "## 6. Multivariate Time Series\n",
    "\n",
    "In multivariate time series analysis, we examine multiple interrelated variables simultaneously instead of just one. This analysis helps us understand how these variables influence each other over time and enables joint forecasting. The methods used for analyzing single time series can often be adapted for multivariate scenarios, but it becomes essential to consider how the interactions between these variables affect their individual behaviors. The following are some specific analyses that demonstrate the exploration of the dynamic relationships and dependencies inherent in multivariate datasets and time series (non-exhaustive)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d9e773-57b4-4b97-84fe-ef2b1a875f2d",
   "metadata": {},
   "source": [
    "### **Example 5: Large Dataset of Synthetic Time Series**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace17745-374e-4bbb-85af-f046f9932edd",
   "metadata": {},
   "source": [
    "We generate multiple correlated time series with trend, seasonal effect and random noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcc054-48bc-404b-8a7e-0d43547de316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic time series data\n",
    "dates = pd.date_range('2000-01-01', periods=100, freq='M')\n",
    "\n",
    "# Create a DataFrame to hold the time series\n",
    "data = pd.DataFrame(index=dates)\n",
    "\n",
    "# Generate time series\n",
    "n_series = 24\n",
    "for i in range(1, n_series + 1):  # 24 time series\n",
    "    trend = np.linspace(0, np.random.uniform(0.1, 1), len(dates))  # Linear trend\n",
    "    seasonal = 0.1 * np.sin(np.linspace(0, 3 * np.pi, len(dates)))  # Seasonal effect\n",
    "    noise = np.random.normal(loc=0, scale=0.05, size=len(dates))  # Random noise\n",
    "    series = trend + seasonal + noise  # Combine to create the time series\n",
    "    data[f'Series_{i}'] = series\n",
    "\n",
    "# Plotting all the time series\n",
    "plt.figure()\n",
    "for column in data.columns:\n",
    "    plt.plot(data.index, data[column], label=column, color='blue')\n",
    "plt.title('Synthetic Time Series Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "plt.grid()\n",
    "plt.tight_layout()  # Adjust the layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753d715-a074-4bbc-8329-945fb5d2dfb4",
   "metadata": {},
   "source": [
    "## 6.1 Pearson Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937d5f58-ae45-455c-b09c-20f5776e4ecf",
   "metadata": {},
   "source": [
    "Pearson correlation measures how strong and in what direction two continuous variables are related. It ranges from -1 to 1: a value of 1 means a perfect positive relationship, -1 means a perfect negative relationship, and 0 means no relationship. It looks at how much one variable changes in relation to another and is commonly used to understand connections between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c7b5e-d479-4c18-9827-f6d321a34f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Pearson correaltion\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# plot heatmap\n",
    "import seaborn as sns # another library for plotting\n",
    "plt.figure()\n",
    "sns.heatmap(correlation_matrix, annot=False, fmt=\".2f\", cmap='Blues', square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Pearson Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Find the two most correlated series\n",
    "corr_values = correlation_matrix.unstack().sort_values(ascending=False)\n",
    "corr_values = corr_values[corr_values < 1.0] # Exclude self-correlations (1.0) \n",
    "most_correlated_pair = corr_values.idxmax()\n",
    "series1, series2 = most_correlated_pair\n",
    "print(f'Most Correlated Series: {series1} and {series2}')\n",
    "\n",
    "# Plotting all the time series, highlighting the two most correlated series\n",
    "plt.figure()\n",
    "for column in data.columns:\n",
    "    plt.plot(data.index, data[column], label=column, color='lightgrey')\n",
    "plt.plot(data.index, data[series1], label=series1, color='blue', linewidth=2.5)  # First highlighted series\n",
    "plt.plot(data.index, data[series2], label=series2, color='blue', linewidth=2.5)  # Second highlighted series\n",
    "plt.title('Synthetic Time Series Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1d370-45e8-4dcf-a2ec-6bdccfa0ea46",
   "metadata": {},
   "source": [
    "## 6.2 Cross-Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d66e5-efb0-4aea-8db0-d25ef65752ff",
   "metadata": {},
   "source": [
    "The cross-correlation function measures how similar two time series are at different time lags. Here, we focus on the cross-correlation between two selected time series and limit our examination to a maximum of 15 lags.\n",
    "\n",
    "The plot reveals a strong correlation at lag 0, indicating that the two series change together simultaneously. As we increase the lag, the correlation slowly decreases but remains relatively high, showing a strong positive relationship that weakens over time. In summary, the plot demonstrates that the time series are closely connected, and this relationship continues even when one series is shifted over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f397361a-6129-4080-942c-01fc34fda853",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lag = 15\n",
    "cross_corr = ccf(data[series1], data[series2])[:max_lag]\n",
    "\n",
    "# Plot Cross-Correlation\n",
    "plt.figure()\n",
    "plt.stem(range(max_lag), cross_corr)\n",
    "plt.title(f\"Cross-Correlation between {series1} and {series2}\")\n",
    "plt.xlabel(\"Lags\")\n",
    "plt.ylabel(\"Cross-Correlation\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3f233f-4d81-4475-8ab2-e2c91a0be4a2",
   "metadata": {},
   "source": [
    "## 6.3 Dimensionality Reduction: Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb97c2-cc83-480a-a505-be97d1df98d2",
   "metadata": {},
   "source": [
    "PCA is one example of a dimensionality reduction technique used in data analysis and machine learning. It transforms a large set of variables into a smaller one while preserving as much variance as possible. By identifying the directions (principal components) in which the data varies the most, PCA can simplify complex datasets including time series.\n",
    "\n",
    "We create a PCA model and specify that we want to keep 5 principal components (PCs). We extract these components from the standardized data (see notebook C3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb93b8-de7f-4e6b-af78-058668cf9ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "\n",
    "# Calculate PCs\n",
    "pca = PCA(n_components=5)  # Adjust number of components if necessary\n",
    "pca_scores = pca.fit_transform(standardized_data)\n",
    "pca_df = pd.DataFrame(data=pca_scores, columns=[f'PC_{i + 1}' for i in range(pca.n_components_)], index=data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f3b3a-f261-4dcf-980b-fd2db66f2693",
   "metadata": {},
   "source": [
    "The explained variance in the PCA describes how much variance (or information) each PC captures. Higher values mean the component is more important. It helps us understand how many components we might need to keep for analysis. Here, already the first PC explains more than 80% of variance in the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04936df5-f072-4265-bc81-f90fdfd9c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the explained variance\n",
    "plt.figure()\n",
    "plt.plot(range(1, 6), pca.explained_variance_ratio_, marker='o')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1992610-39d7-493c-a6b2-17081fc11d63",
   "metadata": {},
   "source": [
    "We visualize the values of the first two PCs over time. This helps us see how the main patterns in the data change. The scatter plot of PC1 versus PC2 visualizes the relationship between the first two principal components, illustrating how observations are distributed in the reduced dimensional space. This analysis allows us to identify clusters, trends, and potential outliers, thereby providing insights into the underlying patterns within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e7e291-b550-4d24-84d4-a86222090e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the first two principal components\n",
    "\n",
    "# time series plot\n",
    "plt.figure()\n",
    "plt.plot(pca_df.index, pca_df['PC_1'], label='Principal Component 1', color='blue')\n",
    "plt.plot(pca_df.index, pca_df['PC_2'], label='Principal Component 2', color='blue', linestyle='--')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()  # Adjust the layout\n",
    "plt.show()\n",
    "\n",
    "# scatter plot\n",
    "plt.figure()\n",
    "sns.scatterplot(data=pca_df, x='PC_1', y='PC_2', color='blue')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bdea6b-9975-4031-9603-01036db3602d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
