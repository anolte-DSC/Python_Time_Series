{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d499fb-f4a0-420a-bd70-7fd16515cb5f",
   "metadata": {},
   "source": [
    "<img src=\"../Images/DSC_Logo.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d42ef55-b242-4201-99ab-12978da76a6c",
   "metadata": {},
   "source": [
    "# Time Series Theory in Python - Part 4: Regression and Advanced Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4548cbc1-1c5e-4b9d-9c26-ad3403c344dc",
   "metadata": {},
   "source": [
    "This notebook demonstrates time series regression and introduces exemplary advanced modeling techniques that can be advantageous for time series prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20d62b-a59e-473d-ab67-8921d9d4567f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install statsmodels PythonTsa statsmodels scikit-learn openpyxl tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f71e86-6bf6-4fac-94cd-a2a0ca18bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PythonTsa.datadir import getdtapath\n",
    "dtapath=getdtapath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735a549-1ab1-400a-b609-0f2c39866698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ff76c1-3d7c-4418-b9e7-8ee3692f3b23",
   "metadata": {},
   "source": [
    "## 1 Time Series Regression Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61122a15-7368-4554-ae8c-ca15f4a07f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06cda99-3204-4442-aed1-619a92042b19",
   "metadata": {},
   "source": [
    "### **Example 1: Australian Employed Total Persons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073d1906-2624-4a72-8514-9def87cbe8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data file path\n",
    "dtapath = getdtapath()\n",
    "\n",
    "# Load the Excel file containing employment data\n",
    "aul = pd.read_excel(dtapath + 'AustraliaEmployedTotalPersons.xlsx', header=0)\n",
    "\n",
    "# Create a time index starting from February 1978 with monthly frequency\n",
    "timeindex = pd.date_range('1978-02', periods=len(aul), freq='ME')\n",
    "aul.index = timeindex\n",
    "\n",
    "# Extracting employed persons\n",
    "y = aul['EmployedP'].values\n",
    "time = np.arange(len(y))  # Create an array of time periods (0, 1, 2, ...)\n",
    "\n",
    "# Fit OLS models\n",
    "linear_model = sm.OLS(y, sm.add_constant(time)).fit()\n",
    "quadratic_model = sm.OLS(y, sm.add_constant(np.column_stack((time, time**2)))).fit()\n",
    "cubic_model = sm.OLS(y, sm.add_constant(np.column_stack((time, time**2, time**3)))).fit()\n",
    "\n",
    "# Extracting the slope and intercept for the linear model\n",
    "slope_LSTM = linear_model.params[1]  \n",
    "intercept_LSTM = linear_model.params[0] \n",
    "print(\"m:\", slope_LSTM, \"n:\", intercept_LSTM)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure()\n",
    "plt.scatter(aul.index, y, label='Total Employed Persons', color='gray', alpha=0.5)\n",
    "\n",
    "# Fitted values from each model\n",
    "plt.plot(aul.index, linear_model.fittedvalues, label='Linear', color='blue')\n",
    "plt.plot(aul.index, quadratic_model.fittedvalues, label='Quadratic', color='black')\n",
    "plt.plot(aul.index, cubic_model.fittedvalues, label='Cubic', color='red', linestyle='--')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Trend Analysis of Total Employed Persons in Australia')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Persons Employed')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print the summary statistics for models\n",
    "print(\"Linear Model Summary:\")\n",
    "print(linear_model.summary())\n",
    "print(\"\\nQuadratic Model Summary:\")\n",
    "print(quadratic_model.summary())\n",
    "print(\"\\nCubic Model Summary:\")\n",
    "print(cubic_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff32b61-d83e-4028-a294-4e658c84ff81",
   "metadata": {},
   "source": [
    "Compare models using AIC and the adjusted R-squared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a5d02b-8da8-4d8d-aa79-ed5bbc94b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel Comparison:\")\n",
    "print(f\"Linear Model - AIC: {linear_model.aic:.2f}, Adjusted R-squared: {linear_model.rsquared_adj:.4f}\")\n",
    "print(f\"Quadratic Model - AIC: {quadratic_model.aic:.2f}, Adjusted R-squared: {quadratic_model.rsquared_adj:.4f}\")\n",
    "print(f\"Cubic Model - AIC: {cubic_model.aic:.2f}, Adjusted R-squared: {cubic_model.rsquared_adj:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b169db-b46b-4c47-9709-2c0d41be00ac",
   "metadata": {},
   "source": [
    "### **Example 2: Mean Spring Passage Dates of European Pied Flycatcher**\n",
    "\n",
    "The dataset contains the migration data of the European Pied Flycatcher, focusing on the adjusted mean spring passage dates (MADJDAYSWS) across multiple years. The dataset includes annual observations, capturing trends in migration timing and the relationship between migration timing and several influential climate variables from key geographic regions.\n",
    "\n",
    "Identified final weather variables that are likely to affect mean spring passage dates at Helgoland of the European Pied Flycatcher:\n",
    "- ID14: mean temperature in a specific region Germany (23 Mar - 14 May)\n",
    "- ID25: number of days with winds coming from Helgoland in a region in Mali (09 Feb - 09 Jun)\n",
    "- ID15: mean temperature in a specific region in Italy (27 Nov - 12 Dec)\n",
    "- ID8: mean temperature in the region of northwestern Algeria and northeastern Marocco (10 Feb - 08 Jun)\n",
    "- ID53: number of days with winds going to Helgoland in a region in Nigeria (16 Apr - 05 May)\n",
    "\n",
    "**Original dataset and code:** Haest, B., Hüppop, O., & Bairlein, F. (2020). Code and data for: \"Weather at the winter and stopover areas determines spring migration onset, progress, and advancements in Afro-Palearctic migrant birds\". In Proceedings of the National Academy of Sciences of the United States of America (v1.0, Bd. 117, Nummer 29, S. 17056–17062). Zenodo. doi:10.5281/zenodo.3629178              \n",
    "\n",
    "**Related publication(s):** Haest, B., Hüppop, O., and Bairlein, F.: Weather at the winter and stopover areas determines spring migration onset, progress, and advancements in Afro-Palearctic migrant birds, Proceedings of the National Academy of Sciences, 117, 17056–17062, doi:10.1073/pnas.1920448117,              2020.\n",
    "\n",
    "Original data and code were modified for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9d768-e4fe-4ed9-bd45-7f130abf1610",
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_migration = pd.read_csv('../Datasets/bird_migration.csv', sep = ';')\n",
    "bird_migration.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5400e7e6-f29e-44ef-a5fb-bb7921248de1",
   "metadata": {},
   "source": [
    "The study fitted regression lines to the mean spring passage dates, modeling the trend in MSPD using just the temporal aspect (e.g., the effect of the year) to understand how the MSPD changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cced472-4f4f-4da7-90c2-fb2b97a0371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the 'MADJDAYSWS' is the column you want as y\n",
    "y2 = bird_migration['MADJDAYSWS'].values  # Extracting dependent variable\n",
    "time2 = bird_migration['Year'].values  # Using the Year column as the time variable\n",
    "\n",
    "# Fit OLS models\n",
    "linear_model = sm.OLS(y2, sm.add_constant(time2)).fit()\n",
    "quadratic_model = sm.OLS(y2, sm.add_constant(np.column_stack((time2, time2**2)))).fit()\n",
    "cubic_model = sm.OLS(y2, sm.add_constant(np.column_stack((time2, time2**2, time2**3)))).fit()\n",
    "\n",
    "# Extracting the slope and intercept for the linear model\n",
    "slope_LSTM = linear_model.params[1]  \n",
    "intercept_LSTM = linear_model.params[0] \n",
    "print(\"m:\", slope_LSTM, \"n:\", intercept_LSTM)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure()\n",
    "plt.scatter(bird_migration['Year'], y2, label='Data')\n",
    "plt.plot(time2, linear_model.fittedvalues, label='Linear', color='blue')\n",
    "plt.plot(time2, quadratic_model.fittedvalues, label='Quadratic', color='black')\n",
    "plt.plot(time2, cubic_model.fittedvalues, label='Cubic', color='red', linestyle='--')\n",
    "plt.legend()\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('MADJDAYSWS')\n",
    "plt.title('Trend Analysis for European Pied Flycatcher')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96f5ee-64ae-46cf-9e23-5d97bcb50720",
   "metadata": {},
   "source": [
    "When comparing models for the mean spring passage dates (MSPD) of migratory birds, AIC helps identify which trend model balances goodness-of-fit with simplicity, where lower values suggest a more efficient model. Additionally, the adjusted R-squared provides information how well the model explains the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442242a-a8fd-41f1-b8c2-a019f91eeb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel Comparison:\")\n",
    "print(f\"Linear Model - AIC: {linear_model.aic:.2f}, Adjusted R-squared: {linear_model.rsquared_adj:.4f}\")\n",
    "print(f\"Quadratic Model - AIC: {quadratic_model.aic:.2f}, Adjusted R-squared: {quadratic_model.rsquared_adj:.4f}\")\n",
    "print(f\"Cubic Model - AIC: {cubic_model.aic:.2f}, Adjusted R-squared: {cubic_model.rsquared_adj:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca66a0df-06fe-4247-b715-ec0ba9b18642",
   "metadata": {},
   "source": [
    "Birds arrive earlier. The linear model yielded the lowest AIC of 329.38, indicating it is the best fit among the three models, despite the relatively low adjusted R-squared of 0.3311, suggesting that approximately 33% of the variability in the migration timing is explained by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42afc6-4432-4567-83c5-3dc24916b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Extracting the dependent variable and the features\n",
    "y2 = bird_migration['MADJDAYSWS'].values  # Target variable\n",
    "X = bird_migration[['ID14', 'ID25', 'ID15', 'ID8', 'ID53']]  # Features\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = linear_model.predict(X_test)\n",
    "\n",
    "# Print model coefficients\n",
    "print(\"Model coefficients:\")\n",
    "for i, col in enumerate(X.columns):\n",
    "    print(f\"{col}: {linear_model.coef_[i]}\")\n",
    "\n",
    "# Calculate permutation feature importance\n",
    "result = permutation_importance(linear_model, X_test, y_test, n_repeats=30, random_state=42)\n",
    "\n",
    "# Get feature importance\n",
    "importance = result.importances_mean\n",
    "indices = np.argsort(importance)[::-1]\n",
    "\n",
    "# Print feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for i in range(X.shape[1]):\n",
    "    print(f\"{i + 1}. Feature {X.columns[indices[i]]} ({importance[indices[i]]})\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure()\n",
    "plt.title(\"Permutation Feature Importance\")\n",
    "plt.bar(range(X.shape[1]), importance[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1e80bb-a5bd-438e-834c-101b3484ef87",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "- ID25: Mali (Wind - Coming From Helgoland, 09 Feb - 09 Jun): Identified as the most important factor for predicting migration timing, with a feature importance score of 0.1162. Good winds from Helgoland at the wintering site help the birds travel faster, making them arrive earlier. Although we did not apply exactly the same methods as the study, this finding aligns with the study's finding that wind conditions, particularly in stopover and wintering sites, are a major influence on spring migration timing.\n",
    "- ID14: Germany (Temperature, 23 Mar - 14 May): The temperature during this time affects when birds migrate, suggesting that warmer temperatures lead to earlier migrations.\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce514ce-7045-482f-ba92-6f4d942a004c",
   "metadata": {},
   "source": [
    "## 2. Advanced Time Series Prediction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce57517f-b191-4b01-824e-b95c19f5ec81",
   "metadata": {},
   "source": [
    "## 2.1 GAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d99ae-75fe-4c93-9f97-d1f67ee896ef",
   "metadata": {},
   "source": [
    "$$E(Y)=β0​+f1​(X1​)+f2​(X2​)+...+fk​(Xk​)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd893e39-0ee4-429f-a712-df92b760ad50",
   "metadata": {},
   "source": [
    "### **Example 1 [continued]: Global Annual Mean Surface Air Temperature Changes Series (1880-1985)**\n",
    "\n",
    "The time series dataset \"Global mean surface air temperature changes 1880–1985\" (denoted as GMSATC) from the folder `Ptsadata` is from Hansen and Lebedeff (1987) that investigates the global warming issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9c340c-f024-44ab-ba1a-22f71c5d999b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from patsy import dmatrix\n",
    "\n",
    "# Load the Excel file containing employment data\n",
    "dtapath = getdtapath()\n",
    "aul = pd.read_excel(dtapath + 'AustraliaEmployedTotalPersons.xlsx', header=0)\n",
    "\n",
    "# Create a time index starting from February 1978 with monthly frequency\n",
    "timeindex = pd.date_range('1978-02', periods=len(aul), freq='ME')\n",
    "aul.index = timeindex\n",
    "\n",
    "# Extracting employed persons\n",
    "y = aul['EmployedP'].values\n",
    "time = np.arange(len(y))  # Create an array of time periods (0, 1, 2, ...)\n",
    "\n",
    "# Fit OLS models\n",
    "linear_model = sm.OLS(y, sm.add_constant(time)).fit()\n",
    "quadratic_model = sm.OLS(y, sm.add_constant(np.column_stack((time, time**2)))).fit()\n",
    "cubic_model = sm.OLS(y, sm.add_constant(np.column_stack((time, time**2, time**3)))).fit()\n",
    "\n",
    "# Fit a Generalized Additive Model (GAM) using B-splines\n",
    "bsplines = dmatrix(\"bs(time, df=6)\", {\"time\": time}, return_type='dataframe')\n",
    "gam_model = sm.GLM(y, bsplines, family=sm.families.Gaussian()).fit()\n",
    "\n",
    "# Plotting results\n",
    "plt.figure()\n",
    "plt.scatter(aul.index, y, label='Total Employed Persons', color='gray', alpha=0.5)\n",
    "\n",
    "# Fitted values from each model\n",
    "plt.plot(aul.index, linear_model.fittedvalues, label='Linear', color='blue')\n",
    "plt.plot(aul.index, quadratic_model.fittedvalues, label='Quadratic', color='black')\n",
    "plt.plot(aul.index, cubic_model.fittedvalues, label='Cubic', color='red', linestyle='--')\n",
    "plt.plot(aul.index, gam_model.fittedvalues, label='GAM', color='green', linewidth=2)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Trend Analysis of Total Employed Persons in Australia')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Persons Employed')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print the summary statistics for models\n",
    "print(\"Linear Model Summary:\")\n",
    "print(linear_model.summary())\n",
    "print(\"\\nQuadratic Model Summary:\")\n",
    "print(quadratic_model.summary())\n",
    "print(\"\\nCubic Model Summary:\")\n",
    "print(cubic_model.summary())\n",
    "print(\"\\nGAM Model Summary:\")\n",
    "print(gam_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a758f58-30c7-41ea-af84-811b9764ab66",
   "metadata": {},
   "source": [
    "## 2.2 Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d15ae-618e-4708-8d24-398ae8cbd7f4",
   "metadata": {},
   "source": [
    "### **Example 2: Hourly Series of Electricity Load**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123c8d0-a091-48f8-87aa-53b19c3c0eff",
   "metadata": {},
   "source": [
    "## 2.2.1 Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707dadd-fa9b-4688-bea4-2abd214f0aec",
   "metadata": {},
   "source": [
    "Preprocessing datasets for machine learning tasks generally involves scaling and splitting the dataset. Datasets are typically divided into three parts: the train set, validation set, and test set. These sets do not overlap. This approach ensures a robust evaluation of the performance of a machine learning model. A two-split approach (training and validation) can be sufficient depending on the research question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d0448b-be42-451a-ad44-51a671d7532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Load the data\n",
    "tsdta = pd.read_csv(dtapath + \"elec-temp.csv\")\n",
    "tsdta['time'] = pd.to_datetime(tsdta['time'])\n",
    "tsdta.set_index('time', inplace=True)\n",
    "\n",
    "# Ensure we are only working with the \"load\" data\n",
    "loadts = tsdta[['load']]  # Assuming the column is named 'load'\n",
    "\n",
    "# Plot for loadts2\n",
    "loadts2 = loadts[loadts.index > '2014-12-18 00:00:00']\n",
    "loadts2.plot(title='Load from 2014-12-18 00:00:00 to present')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Load')\n",
    "plt.show()\n",
    "\n",
    "# Plot for loadts3\n",
    "loadts3 = loadts[(loadts.index > '2012-03-01 00:00:00') & (loadts.index < '2014-03-01 00:00:00')]\n",
    "loadts3.plot(title='Load from 2012-03-01 00:00:00 to 2014-03-01 00:00:00')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Load')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Splitting the datasets\n",
    "validtime = '2014-09-01 00:00:00'\n",
    "testtime = '2014-11-01 00:00:00'\n",
    "\n",
    "loadts_split = (\n",
    "    loadts[(loadts.index < validtime)].rename(columns={'load': 'train'})\n",
    "    .join(loadts[(loadts.index >= validtime) & (loadts.index < testtime)]\n",
    "           .rename(columns={'load': 'validation'}), how='outer')\n",
    "    .join(loadts[testtime:].rename(columns={'load': 'test'}), how='outer')\n",
    ")\n",
    "\n",
    "# Plotting the train, validation, and test sets\n",
    "loadts_split.plot(y=['train', 'validation', 'test'], style=['-', '-.', '--'])\n",
    "plt.ylabel('Electricity load')\n",
    "plt.title('Load Data Train, Validation and Test Split')\n",
    "plt.show()\n",
    "\n",
    "# Train/Validation/Test datasets\n",
    "train = loadts.copy()[loadts.index < validtime]\n",
    "valid = loadts.copy()[(loadts.index >= validtime) & (loadts.index < testtime)]\n",
    "test = loadts.copy()[loadts.index >= testtime]\n",
    "\n",
    "# Scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "train['load'] = scaler.fit_transform(train)\n",
    "valid['load'] = scaler.transform(valid)\n",
    "test['load'] = scaler.transform(test)\n",
    "\n",
    "# ACF\n",
    "plot_acf(train['load'], lags=72)\n",
    "plt.title('ACF of the Generated AR(2) Sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c276c9f-c18b-4d71-9219-7943f335eaaf",
   "metadata": {},
   "source": [
    "In this preprocessing procedure, the dataset is manually split into training, validation, and test sets based on specific timestamps. This method is especially usefull for time series, because it retains the temporal ordering of the data. For specific tasks, the `train_test_split` function can also be employed to randomly partition the dataset into training and testing subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dbd2a3-834f-4c26-bd78-52aa51c91515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming loadts is your DataFrame with a 'load' column\n",
    "# Splitting the dataset randomly into train and test\n",
    "train, temp = train_test_split(loadts, test_size=0.3, random_state=42)  # 70% train and 30% temp\n",
    "valid, test = train_test_split(temp, test_size=0.5, random_state=42)  # Split temp into 50% valid and 50% test\n",
    "\n",
    "# Plotting the train, validation, and test sets\n",
    "plt.figure()\n",
    "plt.scatter(train.index, train['load'], label='Train')\n",
    "plt.scatter(valid.index, valid['load'], label='Validation')\n",
    "plt.scatter(test.index, test['load'], label='Test')\n",
    "plt.ylabel('Electricity load')\n",
    "plt.title('Load Data Train, Validation and Test Split')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "train['load'] = scaler.fit_transform(train[['load']])\n",
    "valid['load'] = scaler.transform(valid[['load']])\n",
    "test['load'] = scaler.transform(test[['load']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec6c87c-e674-49a7-a87c-4550de74f315",
   "metadata": {},
   "source": [
    "## 2.2.2 Build Gated Recurrent Unit (GRU) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08e138-c280-41ec-b9bb-7c5519c8a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the training dataset for the GRU model\n",
    "T = 24  # Lookback period\n",
    "HORIZON = 1  # One-step-ahead prediction\n",
    "\n",
    "train_shifted = train.copy()\n",
    "train_shifted['y_t+1'] = train_shifted['load'].shift(-1, freq='H')\n",
    "\n",
    "for t in range(1, T + 1):\n",
    "    train_shifted[str(T - t)] = train_shifted['load'].shift(T - t, freq='H')\n",
    "\n",
    "y_col = 'y_t+1'\n",
    "X_cols = [f'load_t-{i}' for i in range(23, -1, -1)]\n",
    "train_shifted.columns = ['load_original'] + [y_col] + X_cols\n",
    "train_shifted = train_shifted.dropna(how='any')\n",
    "\n",
    "y_train = np.array(train_shifted[y_col])\n",
    "X_train = np.array(train_shifted[X_cols])\n",
    "X_train = X_train.reshape(X_train.shape[0], T, 1)\n",
    "\n",
    "# Preparing validation and test sets (similar to train_shifted)\n",
    "valid_shifted = valid.copy()\n",
    "test_shifted = test.copy()\n",
    "\n",
    "for data_shifted, dataset in zip([valid_shifted, test_shifted], [valid, test]):\n",
    "    data_shifted['y_t+1'] = data_shifted['load'].shift(-1, freq='H')\n",
    "    for t in range(1, T + 1):\n",
    "        data_shifted[str(T - t)] = data_shifted['load'].shift(T - t, freq='H')\n",
    "    data_shifted.columns = ['load_original'] + [y_col] + X_cols\n",
    "    data_shifted.dropna(how='any', inplace=True)\n",
    "\n",
    "# Prepare y and X for validation and test datasets\n",
    "y_valid, y_test = np.array(valid_shifted[y_col]), np.array(test_shifted[y_col])\n",
    "X_valid = np.array(valid_shifted[X_cols]).reshape(-1, T, 1)\n",
    "X_test = np.array(test_shifted[X_cols]).reshape(-1, T, 1)\n",
    "\n",
    "# Building the GRU model\n",
    "latent_dim = 6\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "\n",
    "model_GRU = Sequential()\n",
    "model_GRU.add(GRU(latent_dim, input_shape=(T, 1)))\n",
    "model_GRU.add(Dense(HORIZON))\n",
    "model_GRU.compile(optimizer='RMSprop', loss='mse')\n",
    "model_GRU.summary()\n",
    "\n",
    "# Training the model\n",
    "model_GRU.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec90db7c-4540-449b-925f-2aa8e274fbf0",
   "metadata": {},
   "source": [
    "## 2.2.3 Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f161e-4e92-4faf-83d1-caf44c49474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you generate predictions called `preds`\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# Create DataFrame for evaluation\n",
    "evdta = pd.DataFrame(preds, columns=[f't+{t}' for t in range(1, HORIZON + 1)])\n",
    "evdta['time'] = test_shifted.index\n",
    "evdta = pd.melt(evdta, id_vars='time', value_name='fitted', var_name='h')\n",
    "evdta['actual'] = np.transpose(y_test).ravel()\n",
    "evdta[['fitted', 'actual']] = scaler.inverse_transform(evdta[['fitted', 'actual']])\n",
    "\n",
    "# Error calculation\n",
    "error = mean_absolute_percentage_error(evdta['actual'], evdta['fitted'])\n",
    "print(f'Mean Absolute Percentage Error: {error}')\n",
    "\n",
    "# Plotting the fitted vs actual values\n",
    "evdta[evdta.time < '2014-11-08'].plot(x='time', y=['fitted', 'actual'], style=['--r', '-b'])\n",
    "plt.ylabel('Electricity load')\n",
    "plt.title('Fitted vs Actual Load Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ccd18e-b6dc-4f92-becd-23b6ee6df918",
   "metadata": {},
   "source": [
    "## 2.2.4 Another NN model: Long-Short-Term Memory (LSTM) \n",
    "\n",
    "To build a first version of another type of Neural Network model (now LSTM models), simply change the following part. In this adjusted code, we replace the GRU layer with an LSTM layer. While both GRUs and LSTMs are types of recurrent neural networks and are effective for sequence prediction problems, LSTMs have the advantage of being able to remember longer sequences due to their architecture. We will not continue with predictions using the LSTM model at this time, as it requires further hyperparameter tuning and experimentation to optimize performance before we can reliably compare its results with the previously established GRU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed5ca2b-1097-48d7-a2b0-cce95ac3fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(LSTM(latent_dim, input_shape=(T, 1)))\n",
    "model_LSTM.add(Dense(HORIZON))\n",
    "model_LSTM.compile(optimizer='RMSprop', loss='mse')\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b9786d-c817-4b41-9a93-fbcb97d27b95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
