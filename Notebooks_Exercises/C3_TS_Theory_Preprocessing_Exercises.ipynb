{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d499fb-f4a0-420a-bd70-7fd16515cb5f",
   "metadata": {},
   "source": [
    "<img src=\"../Images/DSC_Logo.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d42ef55-b242-4201-99ab-12978da76a6c",
   "metadata": {},
   "source": [
    "# Time Series Theory in Python - Part 3: Time Series Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4548cbc1-1c5e-4b9d-9c26-ad3403c344dc",
   "metadata": {},
   "source": [
    "This notebook presents several techniques for time series preprocessing for preparing data for analysis and predictive modeling. These techniques can also serve as independent data analyses on their own and facilitate further preprocessing tailored to specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735a549-1ab1-400a-b609-0f2c39866698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from PythonTsa.datadir import getdtapath\n",
    "dtapath=getdtapath()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000289df-3f96-4c21-893d-f20767337854",
   "metadata": {},
   "source": [
    "## 1. Data Transformation\n",
    "Data transformation encompasses techniques used to modify the data, enhancing its structure and utility for analysis and prediction. This includes fundamental time series operations such as aggregation, where data is summarized or resampled over specified intervals (see Notebook B1), as well as differencing (see Notebook C2). More techniques such as smoothing, detrending, and scaling are introduced below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12d913a-e9c5-4bd4-88cb-5be568b15c3f",
   "metadata": {},
   "source": [
    "## 1.1 Smoothing\n",
    "\n",
    "Smoothing techniques like moving averages reduce noise and highlight trends, making the data easier to analyze and model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff78867f-22ec-403a-a277-876e39410651",
   "metadata": {},
   "source": [
    "### **Example 1: Australian Employed Total Persons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c19bf-4130-4137-90f0-4026d1f9feb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the Excel file containing employment data\n",
    "aul = pd.read_excel(dtapath + 'AustraliaEmployedTotalPersons.xlsx', header=0)\n",
    "\n",
    "# Create a time index starting from February 1978 with monthly frequency\n",
    "timeindex = pd.date_range('1978-02', periods=len(aul), freq='ME')\n",
    "aul.index = timeindex\n",
    "\n",
    "# Extract the 'EmployedP' column for analysis\n",
    "aults = aul['EmployedP']\n",
    "\n",
    "# Apply smoothing using a moving average\n",
    "window_size = 12  # Define the window size for moving average (e.g., 12 for yearly smoothing)\n",
    "aults_smoothed = pd.Series(aults).rolling(window=window_size, center=True).mean().values\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure()\n",
    "plt.scatter(aults.index, aults, label='Original Data', color='gray', alpha=0.5)  # Original data\n",
    "plt.plot(aults.index, aults_smoothed, label='Smoothed Data (Moving Average)', color='blue', linewidth=2)  # Smoothed data\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Original and Smoothed Number of Employed Persons in Australia')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Persons Employed')  # Clarified y-axis label\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c83b887-b416-4a52-b6f6-5d9dc862289a",
   "metadata": {},
   "source": [
    "## 1.2 Removing Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938b522-c280-4122-a3b8-f9630ff99921",
   "metadata": {},
   "source": [
    "In notebook C1, time series decomposition is used to explore the various components of the time series, including trend, seasonality, and residuals. This process not only helps in understanding the underlying structure of the data but can also serve as a preprocessing step for further analysis or predictions. For example, decomposition (introduced in notebook C1 as part of exploratory analysis) or detrending may be employed to make the data stationary or to specifically examine the effects of seasonality and noise. The following example demonstrates detrending which is achieved by fitting a regression model (detailed more in notebook C4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7efc89-3449-4e9a-8bbb-49ac0b11253c",
   "metadata": {},
   "source": [
    "### **Example 2: Mean Spring Passage Dates of European Pied Flycatcher**\n",
    "\n",
    "The dataset contains the migration data of the European Pied Flycatcher, focusing on the adjusted mean spring passage dates (MADJDAYSWS) across multiple years. The dataset includes annual observations.\n",
    "\n",
    "**Original dataset and code:** Haest, B., Hüppop, O., & Bairlein, F. (2020). Code and data for: \"Weather at the winter and stopover areas determines spring migration onset, progress, and advancements in Afro-Palearctic migrant birds\". In Proceedings of the National Academy of Sciences of the United States of America (v1.0, Bd. 117, Nummer 29, S. 17056–17062). Zenodo. doi:10.5281/zenodo.3629178                   \n",
    "\n",
    "**Related publication(s):** Haest, B., Hüppop, O., and Bairlein, F.: Weather at the winter and stopover areas determines spring migration onset, progress, and advancements in Afro-Palearctic migrant birds, Proceedings of the National Academy of Sciences, 117, 17056–17062, doi:10.1073/pnas.1920448117,                   2020.\n",
    "\n",
    "Original data and code were modified for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d6d1ef-1f4f-4368-ba6a-b932f7a7d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_migration = pd.read_csv('../Datasets/bird_migration.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa860b-25f5-40bb-b96a-9fd74b59e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the 'MADJDAYSWS' is the column you want as y\n",
    "y = bird_migration['MADJDAYSWS'].values  # Extracting dependent variable\n",
    "time = bird_migration['Year'].values  # Using the Year column as the time variable\n",
    "\n",
    "# Checking the lengths\n",
    "print(f'Length of y: {len(y)}')\n",
    "print(f'Length of time: {len(time)}')\n",
    "\n",
    "linear_model = sm.OLS(y, sm.add_constant(time)).fit()\n",
    "\n",
    "# Perform linear detrending\n",
    "trend = linear_model.fittedvalues  # This is the linear trend\n",
    "detrended_y = y - trend\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.plot(time, y, label='Original Data', color='gray', alpha=0.5)  # Original data\n",
    "plt.plot(time, trend, label='Linear Trend', color='blue', linewidth=1.5)  # Linear trend\n",
    "plt.plot(time, detrended_y, label='Detrended Data', color='red', linewidth=1.5)\n",
    "plt.title('Original and Detrended MADJDAYSWS')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('MADJDAYSWS')  # Optionally change to 'Detrended MADJDAYSWS'\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd87f10-3dcb-4048-bf46-4fce52fc9baa",
   "metadata": {},
   "source": [
    "## 1.3 Scaling\n",
    "\n",
    "Normalization (min-max scaling) and standardization (Z-score normalization) are used for bringing features onto a similar scale.\n",
    "\n",
    "Note: Most preprocessing functions expect input data to have the shape of (n_samples, n_features). This ensures that the data is correctly interpreted as multiple samples of one or more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50694b5-f5ba-43e2-a4fd-2916a6930200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data for scaling\n",
    "values = y.reshape((len(y), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f1ebce-d1c5-4add-b586-0cd2f0a28632",
   "metadata": {},
   "source": [
    "## 1.3.1 Normalization\n",
    "\n",
    "$$y = (x - min) / (max - min)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c098aa0-9449-4ea3-89a0-35536909967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the normalization\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(values)\n",
    "\n",
    "# normalize the dataset and print the first 5 rows\n",
    "normalized = scaler.transform(values)\n",
    "print(normalized[:5])\n",
    "\n",
    "# inverse transform and print the first 5 rows\n",
    "inversed = scaler.inverse_transform(normalized)\n",
    "print(inversed[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c29c7-883d-488a-83b5-4550511b1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check normalization\n",
    "print(\"Min value normalized:\", min(normalized))\n",
    "print(\"Max value normalized:\", max(normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddcf9c6-b820-496e-b3ac-2e7c917ca16e",
   "metadata": {},
   "source": [
    "## 1.3.2 Standardization\n",
    "\n",
    "$$y = (x - mean) / std$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d88d95c-73f0-438e-8f06-32804c80f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the standardization\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(values)\n",
    "\n",
    "# standardization the dataset and print the first 5 rows\n",
    "normalized = scaler.transform(values)\n",
    "print(normalized[:5])\n",
    "    \n",
    "# inverse transform and print the first 5 rows\n",
    "inversed = scaler.inverse_transform(normalized)\n",
    "print(inversed[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6792ef50-07ec-4f7f-82df-033e36f652f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check standardization\n",
    "print(\"Min value normalized:\", min(normalized))\n",
    "print(\"Max value normalized:\", max(normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601ecec-bb3c-45db-b73e-a419800dcbe9",
   "metadata": {},
   "source": [
    "## 2. Outlier/Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d7ba4-7201-4aef-9ff4-3a7354af14d1",
   "metadata": {},
   "source": [
    "### **Example 3: Artificial Streamflow Time Series with Outliers and Change Point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d08e6-f200-424c-b075-cc9872952230",
   "metadata": {},
   "source": [
    "We create an artificial time series to simulate the dynamics of streamflow over a defined period while incorporating features like outliers and a change point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7203000-34d0-4bb7-95e6-b48524f1d783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "np.random.seed(42)  # For reproducibility\n",
    "days = pd.date_range(start='2021-01-01', end='2023-12-31', freq='D')\n",
    "n = len(days)\n",
    "\n",
    "# Generate a normal streamflow pattern (in m³/s)\n",
    "streamflow = 10 + 0.5 * np.sin(np.linspace(0, 20 * np.pi, n)) + np.random.normal(0, 1, n)\n",
    "\n",
    "# Introduce a change point after 1.5 years (midway through the second year)\n",
    "change_point_index = int(n * 0.75)\n",
    "streamflow[change_point_index:] *= 0.5  # Effect of drying out\n",
    "\n",
    "# Introduce a total of 7 random outliers\n",
    "outliers_indices = np.random.choice(range(n), size=7, replace=False)\n",
    "streamflow[outliers_indices] += np.random.normal(10, 5, size=7)  # Adding outliers above mean\n",
    "\n",
    "# Create a DataFrame\n",
    "sf = pd.DataFrame(data={'Date': days, 'Streamflow (m³/s)': streamflow})\n",
    "\n",
    "# Plot the time series\n",
    "plt.figure()\n",
    "plt.plot(sf['Date'], sf['Streamflow (m³/s)'], label='Streamflow', color='b')\n",
    "plt.axvline(x=sf['Date'][change_point_index], color='r', linestyle='--', label='Change Point')\n",
    "plt.plot(sf['Date'][outliers_indices], sf['Streamflow (m³/s)'][outliers_indices], 'ro', label='Outliers')  # Mark all outliers in red\n",
    "plt.title('Artificial Streamflow Time Series with Outliers and Change Point')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Streamflow (m³/s)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c007b30-9dc5-4fd3-9700-79654f0dff96",
   "metadata": {},
   "source": [
    "## 2.1 Z-Score\n",
    "\n",
    "The Z-score quantifies how many standard deviations a data point is from the mean of a dataset. Outlier are detected by identifying points with Z-scores that exceed a specified threshold (typically ±3), indicating significant deviations from the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86566b51-228c-4477-ad15-b5290d25a205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean = np.mean(streamflow)\n",
    "std_dev = np.std(streamflow)\n",
    "z_scores = (streamflow - mean) / std_dev\n",
    "outliers_z = np.where(np.abs(z_scores) > 3)[0]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sf['Date'], sf['Streamflow (m³/s)'], label='Streamflow', color='b')\n",
    "plt.scatter(sf['Date'][outliers_z], streamflow[outliers_z], color='red', label='Z-score Outliers', zorder=5)\n",
    "plt.title('Z-Score Method for Outlier Detection')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Streamflow (m³/s)')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd170f22-d422-4773-a1c5-67ba195dae7e",
   "metadata": {},
   "source": [
    "## 2.2 Moving Average Method\n",
    "\n",
    "The moving average method is primarily used for smoothing time series data (see 1.1), which can also serve as an effective outlier/anomaly detection method by highlighting deviations from the expected average within a specified time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474d926-5bda-4ae8-b691-787e72288894",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average = pd.Series(streamflow).rolling(window=7).mean()\n",
    "differences = np.abs(streamflow - moving_average)\n",
    "outlier_threshold = differences.mean() + 3 * differences.std()  # Define threshold for outliers\n",
    "outliers_moving_avg = np.where(differences > outlier_threshold)[0]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sf['Date'], streamflow, label='Streamflow', color='b')\n",
    "plt.plot(sf['Date'], moving_average, label='7-Day Moving Average', color='orange')\n",
    "plt.scatter(sf['Date'][outliers_moving_avg], streamflow[outliers_moving_avg], color='red', label='Moving Avg Outliers', zorder=5)\n",
    "plt.title('Moving Average Method for Outlier Detection')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Streamflow (m³/s)')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25396752-feee-474f-bb6d-6918a0826012",
   "metadata": {},
   "source": [
    "## 2.3 Density-based Methods: DBSCAN\n",
    "\n",
    "Density-based outlier methods such as DBSCAN are techniques that identify outliers by analyzing the density of data points in their local neighborhood, flagging those points that reside in low-density regions compared to their high-density surroundings as potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054d4e94-5c83-4479-8723-aae199ef1679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN Outlier Detection\n",
    "scaler = StandardScaler()\n",
    "sf_scaled = scaler.fit_transform(sf[['Streamflow (m³/s)']])  # Scale the data\n",
    "\n",
    "# Fit DBSCAN (adjust eps and min_samples as needed)\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=10)  # eps controls the neighborhood radius, and min_samples controls density threshold\n",
    "sf['Outlier'] = dbscan.fit_predict(sf_scaled)\n",
    "\n",
    "# DBSCAN labels outliers as -1, so we mark them\n",
    "dbscan_outliers = sf[sf['Outlier'] == -1]\n",
    "\n",
    "# Plot the time series with outliers\n",
    "plt.figure()\n",
    "plt.plot(sf['Date'], sf['Streamflow (m³/s)'], label='Streamflow', color='b')\n",
    "plt.plot(dbscan_outliers['Date'], dbscan_outliers['Streamflow (m³/s)'], 'ro', label='DBSCAN Detected Outliers')\n",
    "plt.title('DBSCAN for Outlier Detection')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Streamflow (m³/s)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71356a98-fb45-405b-9ccf-c16a123fb391",
   "metadata": {},
   "source": [
    "## 3. Data Availability and Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da9a5f6-3e99-4ec8-9a66-23990ce95c53",
   "metadata": {},
   "source": [
    "## 3.1 Data Availability in Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c467a-7798-4d8e-abbc-3cb0311c225b",
   "metadata": {},
   "source": [
    "### **Example 4: Large Dataset of Synthetic Time Series with Gaps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc0e03-9848-4e75-8e3b-2bbeb225fe31",
   "metadata": {},
   "source": [
    "We generate multiple synthetic independent time series with trend, seasonal effect, random noise AND gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ab6344-6991-4b12-8732-56eb758586af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic time series data\n",
    "dates = pd.date_range('2000-01-01', periods=100, freq='M')\n",
    "\n",
    "# Create a DataFrame to hold the time series\n",
    "data = pd.DataFrame(index=dates)\n",
    "\n",
    "# Generate time series\n",
    "n_series = 10\n",
    "for i in range(1, n_series + 1):  # 24 time series\n",
    "    trend = np.linspace(0, np.random.uniform(0.1, 1), len(dates))  # Linear trend\n",
    "    seasonal = 0.1 * np.sin(np.linspace(0, 3 * np.pi, len(dates)))  # Seasonal effect\n",
    "    noise = np.random.normal(loc=0, scale=0.05, size=len(dates))  # Random noise\n",
    "    series = trend + seasonal + noise  # Combine to create the time series\n",
    "    data[f'Series_{i}'] = series\n",
    "\n",
    "# Add gaps (missing values) to the series\n",
    "num_gaps = 10  # Number of NaN values to insert in each series\n",
    "for column in data.columns:\n",
    "    indices = np.random.choice(data.index, num_gaps, replace=False)  # Select random indices\n",
    "    data.loc[indices, column] = np.nan  # Assign NaN values\n",
    "data.loc[data.index.year == int(2002), 'Series_2'] = np.nan # one series data gap spans over a total year\n",
    "\n",
    "# Plotting all the time series; highlight the last two series\n",
    "plt.figure()\n",
    "for column in data.columns:\n",
    "    plt.plot(data.index, data[column], label=column, color='blue')\n",
    "plt.plot(data.index, data['Series_2'], label=column, color='red')\n",
    "plt.title('Synthetic Time Series Data with Gaps')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532dd06b-cb95-433f-8dc7-6fb2fb540883",
   "metadata": {},
   "source": [
    "One way to easily analyse data availability in a large number of time series is the heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7fdeb3-f249-479f-879c-31c3d0ff669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for heatmap\n",
    "heatmap_data = data.notna().astype(int)  # Convert to binary (1 for available, 0 for NaN)\n",
    "heatmap_data = heatmap_data.T\n",
    "print(heatmap_data.head())\n",
    "\n",
    "# plot heatmap\n",
    "import seaborn as sns\n",
    "plt.figure()  \n",
    "sns.heatmap(heatmap_data, cmap='binary', cbar_kws={'label': 'Data Availability'}, annot=False)\n",
    "plt.title(\"Heatmap of Data Availability by Sample ID and Date\")\n",
    "plt.xlabel(\"Sample ID\")\n",
    "plt.ylabel(\"Date\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2455e267-a331-4edd-8cf3-bda6fc91f06d",
   "metadata": {},
   "source": [
    "## 3.2 Handling Missing Values: Linear Interpolation\n",
    "\n",
    "Common methods for handling gaps in time series data include forward fill (propagating the last observed value), backward fill (using the next observed value), interpolation (estimating missing values based on surrounding data), and imputation techniques such as mean/median filling. More sophisticated approaches entail using predictive modeling techniques to estimate missing values based on existing patterns in the data. Linear interpolation is one of the simplest and most widely used methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d27b84-3e63-4690-9354-f3a6cb3cceb3",
   "metadata": {},
   "source": [
    "### **Example 3 [continued]: Artificial Streamflow Time Series with Outliers and Change Point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082d77c-a8e5-420d-969b-4a8939a10041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers from the data\n",
    "filtered_streamflow = sf['Streamflow (m³/s)'].copy()\n",
    "filtered_streamflow[outliers_moving_avg] = np.nan  # Set outlier points to NaN\n",
    "\n",
    "# Fill the gaps using linear interpolation\n",
    "filled_streamflow = filtered_streamflow.interpolate(method='linear')\n",
    "\n",
    "# Plotting the original data and the filled data\n",
    "plt.figure()\n",
    "plt.plot(sf['Date'], sf['Streamflow (m³/s)'], label='Original Streamflow', color='b')\n",
    "plt.plot(sf['Date'], filled_streamflow, label='Streamflow after Outlier Removal and Filling', color='orange')\n",
    "plt.scatter(sf['Date'][outliers_moving_avg], streamflow[outliers_moving_avg], color='red', label='Removed Outliers', zorder=5)\n",
    "plt.title('Streamflow Data with Outlier Removal and Filling Gaps')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Streamflow (m³/s)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Zooming in on one of the removed outlier points (e.g., using the first outlier)\n",
    "outlier_to_zoom = outliers_moving_avg[0]  # Choose an example outlier to zoom in\n",
    "start_date = sf['Date'][outlier_to_zoom - 5]  # 5 days before\n",
    "end_date = sf['Date'][outlier_to_zoom + 5]    # 5 days after\n",
    "\n",
    "# zoomed-in\n",
    "plt.figure()\n",
    "plt.plot(sf['Date'], sf['Streamflow (m³/s)'], label='Original Streamflow', color='b', alpha=0.5)\n",
    "plt.plot(sf['Date'], filled_streamflow, label='Streamflow after Outlier Removal and Filling', color='orange')\n",
    "plt.scatter(sf['Date'][outliers_moving_avg], streamflow[outliers_moving_avg], color='red', label='Removed Outliers', zorder=5)\n",
    "plt.xlim([start_date, end_date])\n",
    "plt.title('Zoomed In View Around Removed Outlier')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Streamflow (m³/s)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21de2829-9b05-434f-b65d-48fa4bf8a026",
   "metadata": {},
   "source": [
    "### **Example 4 [continued]: Large Dataset of Synthetic Time Series with Gaps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba178e6-5fc1-498a-9bf9-b37e2d1c662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the gaps using linear interpolation\n",
    "data_filled = data.interpolate(method='linear')\n",
    "\n",
    "# Plotting the original data and the filled data\n",
    "plt.figure()\n",
    "plt.plot(data_filled, label='Data after Filling', color='orange')\n",
    "plt.plot(data, label='Original Data', color='b')\n",
    "plt.title('Data with Filling Gaps')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "handles, labels = plt.gca().get_legend_handles_labels() # Create handles for the legend manually\n",
    "by_label = dict(zip(labels, handles)) # Remove duplicates by turning the labels into a set and getting unique corresponding handles\n",
    "plt.legend(by_label.values(), by_label.keys()) # Create the legend with unique entries\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc723ee0-d038-4eb0-9647-40700d54015e",
   "metadata": {},
   "source": [
    "### Aggregation with Missing Data\n",
    "\n",
    "When data is missing, aggregation methods alone (see notebook B1) cannot adequately provide values for periods without available data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912e8b1e-bfca-445b-a85c-6f34991518d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Series_2'].resample('YE').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618f63e-4a5a-4366-a367-408edeca36aa",
   "metadata": {},
   "source": [
    "In such cases, interpolation at fixed time points can effectively estimate these missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc86454-49af-4b5a-bf96-b790f084fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sequence for interpolation (years from 2000 to 2009)\n",
    "seq_interpol = pd.date_range(start='2000-01-01', end='2009-01-01', freq='YE')\n",
    "\n",
    "# Date data type must be similar to seq_interpol\n",
    "data.index = data.index.astype(np.int64)\n",
    "\n",
    "# Create a new DataFrame to store interpolated values by year\n",
    "interpolated_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each column to perform interpolation\n",
    "for col in data.columns:\n",
    "    \n",
    "    # 1. Drop NaN values for interpolation\n",
    "    df = data[[col]].dropna()\n",
    "    \n",
    "    # 2. Interpolate using linear interpolation\n",
    "    f = interp1d(df.index, df[col], \n",
    "                 kind='linear',  \n",
    "                 bounds_error=False, \n",
    "                 fill_value=np.nan) # no extrapolation\n",
    "\n",
    "    # 3. Get interpolated values for the specific years in seq_interpol\n",
    "    interpolated_values = f(seq_interpol)\n",
    "\n",
    "    # Store result to the DataFrame\n",
    "    interpolated_data[col] = interpolated_values\n",
    "\n",
    "# Set the index of the new DataFrame to the sequence of years\n",
    "interpolated_data.index = seq_interpol\n",
    "\n",
    "# Print results\n",
    "print(interpolated_data)\n",
    "\n",
    "# Plot results\n",
    "plt.figure()\n",
    "plt.plot(interpolated_data, label='Original Data', color='b')\n",
    "plt.plot(interpolated_data.index, interpolated_data['Series_2'], label=column, color='red')\n",
    "plt.title('Yearly interpolated data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadcbea7-39cf-4fd8-905a-506e1bb7aaef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
