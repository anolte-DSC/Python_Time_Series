{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d499fb-f4a0-420a-bd70-7fd16515cb5f",
   "metadata": {},
   "source": [
    "<img src=\"../Images/DSC_Logo.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d42ef55-b242-4201-99ab-12978da76a6c",
   "metadata": {},
   "source": [
    "# Time Series Theory in Python - Part 4: Regression and Advanced Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4548cbc1-1c5e-4b9d-9c26-ad3403c344dc",
   "metadata": {},
   "source": [
    "This notebook demonstrates time series regression and introduces exemplary advanced modeling techniques for time series prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735a549-1ab1-400a-b609-0f2c39866698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "from PythonTsa.datadir import getdtapath\n",
    "dtapath=getdtapath()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ff76c1-3d7c-4418-b9e7-8ee3692f3b23",
   "metadata": {},
   "source": [
    "## 1 Time Series Regression Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06cda99-3204-4442-aed1-619a92042b19",
   "metadata": {},
   "source": [
    "### **Example 1: Australian Employed Total Persons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073d1906-2624-4a72-8514-9def87cbe8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data file path\n",
    "dtapath = getdtapath()\n",
    "\n",
    "# Load the Excel file containing employment data\n",
    "aul = pd.read_excel(dtapath + 'AustraliaEmployedTotalPersons.xlsx', header=0)\n",
    "\n",
    "# Create a time index starting from February 1978 with monthly frequency\n",
    "timeindex = pd.date_range('1978-02', periods=len(aul), freq='ME')\n",
    "aul.index = timeindex\n",
    "\n",
    "# Extracting employed persons\n",
    "y = aul['EmployedP'].values\n",
    "time = np.arange(len(y))  # Create an array of time periods (0, 1, 2, ...)\n",
    "\n",
    "# Fit OLS models\n",
    "linear_model = sm.OLS(y, sm.add_constant(time)).fit()\n",
    "quadratic_model = sm.OLS(y, sm.add_constant(np.column_stack((time, time**2)))).fit()\n",
    "cubic_model = sm.OLS(y, sm.add_constant(np.column_stack((time, time**2, time**3)))).fit()\n",
    "\n",
    "# Extracting the slope and intercept for the linear model\n",
    "slope = linear_model.params[1]  \n",
    "intercept = linear_model.params[0] \n",
    "print(\"m:\", slope, \"n:\", intercept)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure()\n",
    "plt.scatter(aul.index, y, label='Total Employed Persons', color='gray', alpha=0.5)\n",
    "\n",
    "# Fitted values from each model\n",
    "plt.plot(aul.index, linear_model.fittedvalues, label='Linear', color='blue')\n",
    "plt.plot(aul.index, quadratic_model.fittedvalues, label='Quadratic', color='black')\n",
    "plt.plot(aul.index, cubic_model.fittedvalues, label='Cubic', color='red', linestyle='--')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Trend Analysis of Total Employed Persons in Australia')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Persons Employed')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the summary statistics for models\n",
    "print(\"Linear Model Summary:\")\n",
    "print(linear_model.summary())\n",
    "print(\"\\nQuadratic Model Summary:\")\n",
    "print(quadratic_model.summary())\n",
    "print(\"\\nCubic Model Summary:\")\n",
    "print(cubic_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff32b61-d83e-4028-a294-4e658c84ff81",
   "metadata": {},
   "source": [
    "When comparing models, AIC helps identify which trend model balances goodness-of-fit with simplicity, where lower values suggest a more efficient model. Additionally, the adjusted R-squared provides information how well the model explains the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a5d02b-8da8-4d8d-aa79-ed5bbc94b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel Comparison:\")\n",
    "print(f\"Linear Model - AIC: {linear_model.aic:.2f}, Adjusted R-squared: {linear_model.rsquared_adj:.4f}\")\n",
    "print(f\"Quadratic Model - AIC: {quadratic_model.aic:.2f}, Adjusted R-squared: {quadratic_model.rsquared_adj:.4f}\")\n",
    "print(f\"Cubic Model - AIC: {cubic_model.aic:.2f}, Adjusted R-squared: {cubic_model.rsquared_adj:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b169db-b46b-4c47-9709-2c0d41be00ac",
   "metadata": {},
   "source": [
    "### **Example 2: Mean Spring Passage Dates of European Pied Flycatcher**\n",
    "\n",
    "The dataset contains the migration data of the European Pied Flycatcher, focusing on the adjusted mean spring passage dates (MADJDAYSWS) across multiple years. The dataset includes annual observations, capturing trends in migration timing and the relationship between migration timing and several influential climate variables from key geographic regions.\n",
    "\n",
    "Identified final weather variables that are likely to affect mean spring passage dates at Helgoland of the European Pied Flycatcher:\n",
    "- ID14: mean temperature in a specific region Germany (23 Mar - 14 May)\n",
    "- ID25: number of days with winds coming from Helgoland in a region in Mali (09 Feb - 09 Jun)\n",
    "- ID15: mean temperature in a specific region in Italy (27 Nov - 12 Dec)\n",
    "- ID8: mean temperature in the region of northwestern Algeria and northeastern Marocco (10 Feb - 08 Jun)\n",
    "- ID53: number of days with winds going to Helgoland in a region in Nigeria (16 Apr - 05 May)\n",
    "\n",
    "**Original dataset and code:** Haest, B., Hüppop, O., & Bairlein, F. (2020). Code and data for: \"Weather at the winter and stopover areas determines spring migration onset, progress, and advancements in Afro-Palearctic migrant birds\". In Proceedings of the National Academy of Sciences of the United States of America (v1.0, Bd. 117, Nummer 29, S. 17056–17062). Zenodo. doi:10.5281/zenodo.3629178              \n",
    "\n",
    "**Related publication(s):** Haest, B., Hüppop, O., and Bairlein, F.: Weather at the winter and stopover areas determines spring migration onset, progress, and advancements in Afro-Palearctic migrant birds, Proceedings of the National Academy of Sciences, 117, 17056–17062, doi:10.1073/pnas.1920448117,              2020.\n",
    "\n",
    "Original data and code were modified for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9d768-e4fe-4ed9-bd45-7f130abf1610",
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_migration = pd.read_csv('../Datasets/bird_migration.csv', sep = ';')\n",
    "bird_migration.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5400e7e6-f29e-44ef-a5fb-bb7921248de1",
   "metadata": {},
   "source": [
    "The study fitted regression lines to the mean spring passage dates, modeling the trend in MSPD using just the temporal aspect (e.g., the effect of the year) to understand how the MSPD changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cced472-4f4f-4da7-90c2-fb2b97a0371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the 'MADJDAYSWS' is the column we want as y\n",
    "y2 = bird_migration['MADJDAYSWS'].values  # Extracting dependent variable\n",
    "time2 = bird_migration['Year'].values  # Using the Year column as the time variable\n",
    "\n",
    "# Fit OLS models\n",
    "linear_model = sm.OLS(y2, sm.add_constant(time2)).fit()\n",
    "quadratic_model = sm.OLS(y2, sm.add_constant(np.column_stack((time2, time2**2)))).fit()\n",
    "cubic_model = sm.OLS(y2, sm.add_constant(np.column_stack((time2, time2**2, time2**3)))).fit()\n",
    "\n",
    "# Extracting the slope and intercept for the linear model\n",
    "slope = linear_model.params[1]  \n",
    "intercept = linear_model.params[0] \n",
    "print(\"m:\", slope, \"n:\", intercept)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure()\n",
    "plt.scatter(bird_migration['Year'], y2, label='Data', color='gray', alpha=0.5)\n",
    "plt.plot(time2, linear_model.fittedvalues, label='Linear', color='blue')\n",
    "plt.plot(time2, quadratic_model.fittedvalues, label='Quadratic', color='black')\n",
    "plt.plot(time2, cubic_model.fittedvalues, label='Cubic', color='red', linestyle='--')\n",
    "plt.legend()\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('MADJDAYSWS')\n",
    "plt.title('Trend Analysis for European Pied Flycatcher')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442242a-a8fd-41f1-b8c2-a019f91eeb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel Comparison:\")\n",
    "print(f\"Linear Model - AIC: {linear_model.aic:.2f}, Adjusted R-squared: {linear_model.rsquared_adj:.4f}\")\n",
    "print(f\"Quadratic Model - AIC: {quadratic_model.aic:.2f}, Adjusted R-squared: {quadratic_model.rsquared_adj:.4f}\")\n",
    "print(f\"Cubic Model - AIC: {cubic_model.aic:.2f}, Adjusted R-squared: {cubic_model.rsquared_adj:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca66a0df-06fe-4247-b715-ec0ba9b18642",
   "metadata": {},
   "source": [
    "Birds arrive earlier. The linear model yielded the lowest AIC of 329.38, indicating it is the best fit among the three models, despite the relatively low adjusted R-squared of 0.3311, suggesting that approximately 33% of the variability in the migration timing is explained by the model. \n",
    "\n",
    "### Linear regression with explanatory variables\n",
    "\n",
    "The following code block prepares the data for a prediction task where MSPD shall be predicted using five pre-selected explanatory variables. \n",
    "\n",
    "The dataset is splitted into a training set for fitting the regression model and a testing set for evaluating its performance using `train_test_split`. It is generally recommended to use a **training, validation, and test split** for most types of predictive modeling. Datasets are typically divided into three parts: the train set, validation set, and test set. These sets do not overlap. This approach ensures a robust evaluation of the performance of model in unseen data. A two-split approach (training and validation) can be sufficient depending on the research question. \n",
    "\n",
    "After fitting the Linear Regression model to the training data using `LinearRegression`, predictions are made on the test set. The coefficients of the model are printed, providing insights into the relationship between the features and the target variable. **Feature importance** is analyzed using `permutation_importance` as an example. The permutation feature importance  allows us to assess the impact of each feature on the model's predictive performance by randomly shuffling the feature values and measuring the change in prediction accuracy. For the permutation feature importance, it is important that the features are uncorrelated with each other. \n",
    "\n",
    "Note: Both `OLS` (statsmodels library) and `LinearRegression` (scikit-learn library) implement the ordinary least squares method, leading to similar linear regression results (i.e., the estimated coefficients will be the same in theory). However, the way they present the results and the additional functionalities they offer (statistical diagnostics vs. prediction/machine learning workflow) differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42afc6-4432-4567-83c5-3dc24916b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the dependent variable and the features\n",
    "y2 = bird_migration['MADJDAYSWS'].values  # Target variable\n",
    "X = bird_migration[['ID14', 'ID25', 'ID15', 'ID8', 'ID53']]  # Features\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = linear_model.predict(X_test)\n",
    "\n",
    "# Print model coefficients\n",
    "print(\"Model coefficients:\")\n",
    "for i, col in enumerate(X.columns):\n",
    "    print(f\"{col}: {linear_model.coef_[i]}\")\n",
    "\n",
    "# Calculate permutation feature importance\n",
    "result = permutation_importance(linear_model, X_test, y_test, n_repeats=30, random_state=42)\n",
    "\n",
    "# Get feature importance\n",
    "importance = result.importances_mean\n",
    "indices = np.argsort(importance)[::-1]\n",
    "\n",
    "# Print feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for i in range(X.shape[1]):\n",
    "    print(f\"{i + 1}. Feature {X.columns[indices[i]]} ({importance[indices[i]]})\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure()\n",
    "plt.title(\"Permutation Feature Importance\")\n",
    "plt.bar(range(X.shape[1]), importance[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1e80bb-a5bd-438e-834c-101b3484ef87",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "- ID25: Mali (Wind - Coming From Helgoland, 09 Feb - 09 Jun): Identified as the most important factor for predicting migration timing, with a feature importance score of 0.1162. Good winds from Helgoland at the wintering site help the birds travel faster, making them arrive earlier. Although we did not apply exactly the same methods as the study, this finding aligns with the study's finding that wind conditions, particularly in stopover and wintering sites, have a major influence on spring migration timing.\n",
    "- ID14: Germany (Temperature, 23 Mar - 14 May): The temperature during this time affects when birds migrate, suggesting that warmer temperatures lead to earlier migrations.\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce514ce-7045-482f-ba92-6f4d942a004c",
   "metadata": {},
   "source": [
    "## 2. Advanced Time Series Prediction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce57517f-b191-4b01-824e-b95c19f5ec81",
   "metadata": {},
   "source": [
    "## 2.1 GAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d99ae-75fe-4c93-9f97-d1f67ee896ef",
   "metadata": {},
   "source": [
    "$$E(Y)=β0​+f1​(X1​)+f2​(X2​)+...+fk​(Xk​)$$\n",
    "\n",
    "\n",
    "The Generalized Additive Model (GAM) employs flexible B-splines to create smooth fits for data, enabling it to capture non-linear relationships. This capacity allows GAMs to identify complex trends and patterns that simpler models might overlook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd893e39-0ee4-429f-a712-df92b760ad50",
   "metadata": {},
   "source": [
    "### **Example 1 [continued]: Australian Employed Total Persons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9c340c-f024-44ab-ba1a-22f71c5d999b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from patsy import dmatrix \n",
    "\n",
    "# Load the Excel file containing employment data\n",
    "dtapath = getdtapath()\n",
    "aul = pd.read_excel(dtapath + 'AustraliaEmployedTotalPersons.xlsx', header=0)\n",
    "\n",
    "# Create a time index starting from February 1978 with monthly frequency\n",
    "timeindex = pd.date_range('1978-02', periods=len(aul), freq='ME')\n",
    "aul.index = timeindex\n",
    "\n",
    "# Extracting employed persons\n",
    "y = aul['EmployedP'].values\n",
    "time = np.arange(len(y))  # Create an array of time periods (0, 1, 2, ...)\n",
    "\n",
    "# Fit OLS models\n",
    "linear_model = sm.OLS(y, sm.add_constant(time)).fit()\n",
    "quadratic_model = sm.OLS(y, sm.add_constant(np.column_stack((time, time**2)))).fit()\n",
    "cubic_model = sm.OLS(y, sm.add_constant(np.column_stack((time, time**2, time**3)))).fit()\n",
    "\n",
    "# Fit a Generalized Additive Model (GAM) using B-splines\n",
    "bsplines = dmatrix(\"bs(time, df=6)\", {\"time\": time}, return_type='dataframe')\n",
    "gam_model = sm.GLM(y, bsplines, family=sm.families.Gaussian()).fit()\n",
    "\n",
    "# Plotting results\n",
    "plt.figure()\n",
    "plt.scatter(aul.index, y, label='Total Employed Persons', color='gray', alpha=0.5)\n",
    "\n",
    "# Fitted values from each model\n",
    "plt.plot(aul.index, linear_model.fittedvalues, label='Linear', color='blue')\n",
    "plt.plot(aul.index, quadratic_model.fittedvalues, label='Quadratic', color='black')\n",
    "plt.plot(aul.index, cubic_model.fittedvalues, label='Cubic', color='red', linestyle='--')\n",
    "plt.plot(aul.index, gam_model.fittedvalues, label='GAM', color='green', linewidth=2)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Trend Analysis of Total Employed Persons in Australia')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Persons Employed')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print the summary statistics for models\n",
    "print(\"Linear Model Summary:\")\n",
    "print(linear_model.summary())\n",
    "print(\"\\nQuadratic Model Summary:\")\n",
    "print(quadratic_model.summary())\n",
    "print(\"\\nCubic Model Summary:\")\n",
    "print(cubic_model.summary())\n",
    "print(\"\\nGAM Model Summary:\")\n",
    "print(gam_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a758f58-30c7-41ea-af84-811b9764ab66",
   "metadata": {},
   "source": [
    "## 2.2 Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d15ae-618e-4708-8d24-398ae8cbd7f4",
   "metadata": {},
   "source": [
    "### **Example 3: Hourly Series of Electricity Load**\n",
    "\n",
    "The load series has at least three seasonal patterns: hourly (24 hours), monthly (12 months), and quarterly (4 quarters). Because of this complexity, fitting a traditional statistical model to the load series is challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123c8d0-a091-48f8-87aa-53b19c3c0eff",
   "metadata": {},
   "source": [
    "## 2.2.1 Dataset Preprocessing and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707dadd-fa9b-4688-bea4-2abd214f0aec",
   "metadata": {},
   "source": [
    "Preprocessing and preparing datasets for machine learning tasks generally involves scaling and splitting the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d0448b-be42-451a-ad44-51a671d7532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "tsdta = pd.read_csv(dtapath + \"elec-temp.csv\")\n",
    "tsdta['time'] = pd.to_datetime(tsdta['time'])\n",
    "tsdta.set_index('time', inplace=True)\n",
    "\n",
    "# Select \"load\" data\n",
    "loadts = tsdta[['load']]  # Assuming the column is named 'load'\n",
    "\n",
    "# Plot to see hourly seasonality\n",
    "loadts2 = loadts[loadts.index > '2014-12-18 00:00:00']\n",
    "loadts2.plot(title='Load from 2014-12-18 00:00:00 to present')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Load')\n",
    "plt.show()\n",
    "\n",
    "# Plot to see monthly and quarterly seasonality\n",
    "loadts3 = loadts[(loadts.index > '2012-03-01 00:00:00') & (loadts.index < '2014-03-01 00:00:00')]\n",
    "loadts3.plot(title='Load from 2012-03-01 00:00:00 to 2014-03-01 00:00:00')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Load')\n",
    "plt.show()\n",
    "\n",
    "# Splitting the datasets\n",
    "validtime = '2014-09-01 00:00:00'\n",
    "testtime = '2014-11-01 00:00:00'\n",
    "\n",
    "loadts_split = (\n",
    "    loadts[(loadts.index < validtime)].rename(columns={'load': 'train'})\n",
    "    .join(loadts[(loadts.index >= validtime) & (loadts.index < testtime)]\n",
    "           .rename(columns={'load': 'validation'}), how='outer')\n",
    "    .join(loadts[testtime:].rename(columns={'load': 'test'}), how='outer')\n",
    ")\n",
    "\n",
    "# Plotting the train, validation, and test sets\n",
    "loadts_split.plot(y=['train', 'validation', 'test'], style=['-', '-.', '--'])\n",
    "plt.ylabel('Electricity load')\n",
    "plt.title('Load Data Train, Validation and Test Split')\n",
    "plt.show()\n",
    "\n",
    "# Train/Validation/Test datasets\n",
    "train = loadts.copy()[loadts.index < validtime]\n",
    "valid = loadts.copy()[(loadts.index >= validtime) & (loadts.index < testtime)]\n",
    "test = loadts.copy()[loadts.index >= testtime]\n",
    "\n",
    "# Scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "train['load'] = scaler.fit_transform(train)\n",
    "valid['load'] = scaler.transform(valid)\n",
    "test['load'] = scaler.transform(test)\n",
    "\n",
    "# ACF of scaled data: still nonstationary\n",
    "plot_acf(train['load'], lags=72)\n",
    "plt.title('ACF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c276c9f-c18b-4d71-9219-7943f335eaaf",
   "metadata": {},
   "source": [
    "In this preprocessing procedure, the dataset is manually split into training, validation, and test sets based on specific timestamps. This method is especially usefull for time series, because it retains the temporal ordering of the data. In contrast, the `train_test_split` function randomly partitions the dataset into two subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dbd2a3-834f-4c26-bd78-52aa51c91515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset randomly into train and test\n",
    "train2, temp2 = train_test_split(loadts, test_size=0.3, random_state=42)  # 70% train and 30% temp\n",
    "valid2, test2 = train_test_split(temp2, test_size=0.5, random_state=42)  # Split remaining temp into 50% validation and 50% test\n",
    "\n",
    "# Plotting the train, validation, and test sets\n",
    "plt.figure()\n",
    "plt.scatter(train2.index, train2['load'], label='Train', s=5)\n",
    "plt.scatter(valid2.index, valid2['load'], label='Validation', s=5)\n",
    "plt.scatter(test2.index, test2['load'], label='Test', s=5)\n",
    "plt.ylabel('Electricity load')\n",
    "plt.title('Load Data Train, Validation and Test Split')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "train2['load'] = scaler.fit_transform(train2[['load']])\n",
    "valid2['load'] = scaler.transform(valid2[['load']])\n",
    "test2['load'] = scaler.transform(test2[['load']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec6c87c-e674-49a7-a87c-4550de74f315",
   "metadata": {},
   "source": [
    "## 2.2.2 Build model: Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08e138-c280-41ec-b9bb-7c5519c8a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the training dataset for the GRU model\n",
    "T = 24  # Lookback period\n",
    "HORIZON = 1  # One-step-ahead prediction\n",
    "\n",
    "train_shifted = train.copy()\n",
    "train_shifted['y_t+1'] = train_shifted['load'].shift(-1, freq='h')\n",
    "\n",
    "for t in range(1, T + 1):\n",
    "    train_shifted[str(T - t)] = train_shifted['load'].shift(T - t, freq='h')\n",
    "\n",
    "y_col = 'y_t+1'\n",
    "X_cols = [f'load_t-{i}' for i in range(23, -1, -1)]\n",
    "train_shifted.columns = ['load_original'] + [y_col] + X_cols\n",
    "train_shifted = train_shifted.dropna(how='any')\n",
    "\n",
    "y_train = np.array(train_shifted[y_col])\n",
    "X_train = np.array(train_shifted[X_cols])\n",
    "X_train = X_train.reshape(X_train.shape[0], T, 1)\n",
    "\n",
    "# Preparing validation and test sets (similar to train_shifted)\n",
    "valid_shifted = valid.copy()\n",
    "test_shifted = test.copy()\n",
    "for data_shifted, dataset in zip([valid_shifted, test_shifted], [valid, test]):\n",
    "    data_shifted['y_t+1'] = data_shifted['load'].shift(-1, freq='h')\n",
    "    for t in range(1, T + 1):\n",
    "        data_shifted[str(T - t)] = data_shifted['load'].shift(T - t, freq='h')\n",
    "    data_shifted.columns = ['load_original'] + [y_col] + X_cols\n",
    "    data_shifted.dropna(how='any', inplace=True)\n",
    "\n",
    "# Prepare y and X for validation and test datasets\n",
    "y_valid, y_test = np.array(valid_shifted[y_col]), np.array(test_shifted[y_col])\n",
    "X_valid = np.array(valid_shifted[X_cols]).reshape(-1, T, 1)\n",
    "X_test = np.array(test_shifted[X_cols]).reshape(-1, T, 1)\n",
    "\n",
    "# Building the GRU model\n",
    "latent_dim = 6\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "\n",
    "model_GRU = Sequential()\n",
    "model_GRU.add(GRU(latent_dim, input_shape=(T, 1)))\n",
    "model_GRU.add(Dense(HORIZON))\n",
    "model_GRU.compile(optimizer='RMSprop', loss='mse')\n",
    "model_GRU.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9045ca0-ef94-4473-98ff-a9b4c51f9a22",
   "metadata": {},
   "source": [
    "## 2.2.3 Fit (train) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f4e99-8ac0-4faa-9f9a-d29ff2223f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec90db7c-4540-449b-925f-2aa8e274fbf0",
   "metadata": {},
   "source": [
    "## 2.2.4 Predict model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f161e-4e92-4faf-83d1-caf44c49474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "preds = model_GRU.predict(X_test)\n",
    "\n",
    "# Create DataFrame for evaluation\n",
    "evdta = pd.DataFrame(preds, columns=[f't+{t}' for t in range(1, HORIZON + 1)])\n",
    "evdta['time'] = test_shifted.index\n",
    "evdta = pd.melt(evdta, id_vars='time', value_name='fitted', var_name='h')\n",
    "evdta['actual'] = np.transpose(y_test).ravel()\n",
    "evdta[['fitted', 'actual']] = scaler.inverse_transform(evdta[['fitted', 'actual']])\n",
    "\n",
    "# Error calculation\n",
    "error = mean_absolute_percentage_error(evdta['actual'], evdta['fitted'])\n",
    "print(f'Mean Absolute Percentage Error: {error}')\n",
    "\n",
    "# Plotting the fitted vs actual values\n",
    "evdta[evdta.time < '2014-11-08'].plot(x='time', y=['fitted', 'actual'], style=['--r', '-b'])\n",
    "plt.ylabel('Electricity load')\n",
    "plt.title('Fitted vs Actual Load Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ccd18e-b6dc-4f92-becd-23b6ee6df918",
   "metadata": {},
   "source": [
    "## 2.2.5 Another NN model: Long-Short-Term Memory (LSTM) \n",
    "\n",
    "To build a first version of another type of neural network model (now LSTM), simply change the following part in the model building. In this adjusted code, we replace the GRU layer with an LSTM layer. While both GRUs and LSTMs are types of recurrent neural networks and are effective for sequence prediction problems, LSTMs have the advantage of being able to remember longer sequences due to their architecture. We will not continue with predictions using the LSTM model at this time, as it requires further hyperparameter tuning and experimentation to optimize performance before we can reliably compare its results with the previously established GRU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed5ca2b-1097-48d7-a2b0-cce95ac3fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(LSTM(latent_dim, input_shape=(T, 1)))\n",
    "model_LSTM.add(Dense(HORIZON))\n",
    "model_LSTM.compile(optimizer='RMSprop', loss='mse')\n",
    "model_LSTM.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
